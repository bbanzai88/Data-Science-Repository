{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNiRlmUP+Ylvg2QGJCZZecM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25aac677146e483882d2b256cee3b384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a708ec811054dc8927e93f0816ea96a",
              "IPY_MODEL_4cdf667b6f104f1788fea46b66bd5236",
              "IPY_MODEL_2126e3d69d1143dc81c33c5ffe96f427"
            ],
            "layout": "IPY_MODEL_e89c4bcbbb8c4e9782f46c41cfe5cc6f"
          }
        },
        "2a708ec811054dc8927e93f0816ea96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe090794106437491226c9911b26f9c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_beeee5eeb1684419836947fe0b9ce30b",
            "value": "ðŸ“šâ€‡Generatingâ€‡Chapters:â€‡100%"
          }
        },
        "4cdf667b6f104f1788fea46b66bd5236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3b263bb76c4961a28ba8e0373089f5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c49d2de01c694393b429b39072422d4e",
            "value": 4
          }
        },
        "2126e3d69d1143dc81c33c5ffe96f427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48dd4954f614ad39151a2adf5b87858",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_12007f64745c4b43a7d8c00b3b92ad2d",
            "value": "â€‡4/4â€‡[01:27&lt;00:00,â€‡22.19s/it]"
          }
        },
        "e89c4bcbbb8c4e9782f46c41cfe5cc6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe090794106437491226c9911b26f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beeee5eeb1684419836947fe0b9ce30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab3b263bb76c4961a28ba8e0373089f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c49d2de01c694393b429b39072422d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e48dd4954f614ad39151a2adf5b87858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12007f64745c4b43a7d8c00b3b92ad2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbanzai88/Data-Science-Repository/blob/main/Book_Writing_Crew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouFQIforUT60"
      },
      "outputs": [],
      "source": [
        "#This is my initial CrewAI agent.  This is a work in progress..I changed the original so I could use both colab and ollama\n",
        "#together and use an initial prompt. The original version of it is from here:https://github.com/Aftabbs/Book-Writing-AI-Agent/blob/main/Book_Writing_AI_Agent.ipynb.\n",
        "#I also modified it so that it only saves the actual story as output and not the agents notes/thought processes.. Each chapter is  now between 5k-10k."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove embedchain and its old chromadb/tokenizers\n",
        "!pip uninstall -y embedchain chromadb tokenizers\n",
        "\n",
        "# 2. Install only the packages you need + the correct tokenizers\n",
        "!pip install \\\n",
        "  crewai crewai_tools langchain_community PyGithub ollama langchain \\\n",
        "  tokenizers==0.21.2 chromadb\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7ZaadzDuUZBz",
        "outputId": "290b8f05-38fe-47bd-ab6c-879aa0eb8f68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping embedchain as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: chromadb 1.0.15\n",
            "Uninstalling chromadb-1.0.15:\n",
            "  Successfully uninstalled chromadb-1.0.15\n",
            "Found existing installation: tokenizers 0.21.2\n",
            "Uninstalling tokenizers-0.21.2:\n",
            "  Successfully uninstalled tokenizers-0.21.2\n",
            "Requirement already satisfied: crewai in /usr/local/lib/python3.11/dist-packages (0.141.0)\n",
            "Requirement already satisfied: crewai_tools in /usr/local/lib/python3.11/dist-packages (0.0.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting tokenizers==0.21.2\n",
            "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "ent already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0->crewai) (24.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.30.0->crewai) (0.56b0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai) (11.2.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.4->crewai) (43.0.3)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.4.1)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (4.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (3.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n",
            "Requirement already satisfied: ghp-import>=1.0 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (2.1.0)\n",
            "Requirement already satisfied: markdown>=3.3.6 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (3.8.2)\n",
            "Requirement already satisfied: mergedeep>=1.3.4 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (1.3.4)\n",
            "Requirement already satisfied: mkdocs-get-deps>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (0.2.0)\n",
            "Requirement already satisfied: pathspec>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (0.12.1)\n",
            "Requirement already satisfied: pyyaml-env-tag>=0.1 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (1.1)\n",
            "Requirement already satisfied: watchdog>=2.0 in /usr/local/lib/python3.11/dist-packages (from mkdocs>=1.6.1->instructor>=1.3.3->crewai) (6.0.0)\n",
            "Requirement already satisfied: babel~=2.10 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (2.17.0)\n",
            "Requirement already satisfied: backrefs~=5.7.post1 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (5.9)\n",
            "Requirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (0.4.6)\n",
            "Requirement already satisfied: mkdocs-material-extensions~=1.3 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: paginate~=0.5 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (0.5.7)\n",
            "Requirement already satisfied: pymdown-extensions~=10.2 in /usr/local/lib/python3.11/dist-packages (from mkdocs-material>=9.5.49->instructor>=1.3.3->crewai) (10.16)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit>=4.2.0->instructor>=1.3.3->crewai) (3.4.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit>=4.2.0->instructor>=1.3.3->crewai) (2.6.12)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from pre-commit>=4.2.0->instructor>=1.3.3->crewai) (1.9.1)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.11/dist-packages (from pre-commit>=4.2.0->instructor>=1.3.3->crewai) (20.31.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime==1.22.0->crewai) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime==1.22.0->crewai) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from mkdocs-get-deps>=0.2.0->mkdocs>=1.6.1->instructor>=1.3.3->crewai) (4.3.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.2.0->instructor>=1.3.3->crewai) (0.3.9)\n",
            "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Using cached chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "Installing collected packages: tokenizers, chromadb\n",
            "Successfully installed chromadb-1.0.15 tokenizers-0.21.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-1373127563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2. Install only the packages you need + the correct tokenizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install    crewai crewai_tools langchain_community PyGithub ollama langchain    tokenizers==0.21.2 chromadb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mmake_file\u001b[0;34m(name, hash, size_str)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmake_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackagePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mPurePath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPureWindowsPath\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mPurePosixPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# right flavour.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    500\u001b[0m                         \u001b[0;34m\"object returning str, not %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                         % type(a))\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mparse_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, tokenizers, chromadb\n",
        "print(transformers.__version__, tokenizers.__version__, chromadb.__version__)\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2oUHcxFNnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# install system deps and Ollama CLI\n",
        "sudo apt-get update -y\n",
        "sudo apt-get install -y pciutils\n",
        "\n",
        "# this is Ollamaâ€™s official installer\n",
        "curl https://ollama.ai/install.sh | sh\n"
      ],
      "metadata": {
        "id": "Ig9WJE23YsiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, threading, subprocess, time\n",
        "\n",
        "# allow any origin and bind on all interfaces\n",
        "os.environ[\"OLLAMA_HOST\"]    = \"0.0.0.0:11434\"\n",
        "os.environ[\"OLLAMA_ORIGINS\"] = \"*\"\n",
        "\n",
        "def _serve_ollama():\n",
        "    # this spins up the HTTP API at port 11434\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# start the server on a daemon thread\n",
        "threading.Thread(target=_serve_ollama, daemon=True).start()\n",
        "\n",
        "# give it a few seconds to boot\n",
        "time.sleep(10)\n",
        "print(\"âœ… Ollama server should be up at localhost:11434\")\n"
      ],
      "metadata": {
        "id": "ZHtur3QPZN3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull deepseek-r1:1.5b\n"
      ],
      "metadata": {
        "id": "km6dVj5SZSXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama langchain-ollama"
      ],
      "metadata": {
        "id": "SauKWQHdZacp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "36TUyV5mo7NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K0wH31UCvEQ"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Install Ollama if you havenâ€™t already\n",
        "pip install --quiet ollama\n",
        "\n",
        "# Start Ollama in the true background with nohup\n",
        "nohup ollama serve > /content/ollama.log 2>&1 &\n",
        "\n",
        "# Give it a moment to spin up\n",
        "sleep 10\n",
        "\n",
        "# Show the last few lines of the log to confirm itâ€™s listening\n",
        "tail -n 20 /content/ollama.log\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Clear any previous overrides\n",
        "os.environ.pop(\"OLLAMA_HOST\", None)\n",
        "os.environ.pop(\"OLLAMA_ORIGINS\", None)\n",
        "\n",
        "# For CrewFlowâ€™s nested asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Trick OpenAI imports so they don't error\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Longer timeout\n",
        "os.environ[\"LITELLM_TIMEOUT\"] = \"1200\"\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "# Initialize the shared Ollama client\n",
        "ollama_llm = OllamaLLM(\n",
        "    model=\"deepseek-r1:1.5b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",  # must match the server\n",
        "    temperature=0\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "wLOuAXwaCu_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In a Colab cell:\n",
        "!pip install --quiet nest_asyncio\n"
      ],
      "metadata": {
        "id": "M7ddCW52Be66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "2YXlFNH3ByQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes via Ollama\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice via Ollama\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "    json_str = match.group(1)\n",
        "    json_str = re.sub(r',\\s*([}\\]])', '\\\\1', json_str)\n",
        "    json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        json_str = re.sub(r'(\\w+)\\s*:', r'\"\\\\1\":', json_str)\n",
        "        return json.loads(json_str)\n",
        "\n",
        "# Initialize Agents with LiteLLM provider strings\n",
        "specs_agent = Agent(\n",
        "    role='SpecificationsAgent',\n",
        "    goal='Extract story specs (genre, theme, tone, word_count) as JSON',\n",
        "    backstory='Produce a JSON object with keys genre, theme, tone, word_count from a user idea.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "prod_agent = Agent(\n",
        "    role='ProductionAgent',\n",
        "    goal='Generate chapter content and implement creative changes via guidelines',\n",
        "    backstory='Generate full chapter text targeting 2000-4000 words based on outline and specs.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "eval_agent = Agent(\n",
        "    role='EvaluationAgent',\n",
        "    goal='Evaluate chapter drafts and return JSON with score and feedback',\n",
        "    backstory='Return JSON {\"score\":0-1, \"feedback\":\"...\"} evaluating chapter quality and adherence.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "redac_agent = Agent(\n",
        "    role='RedacteurAgent',\n",
        "    goal='Polish chapter prose and maintain consistency',\n",
        "    backstory='Take raw chapter text and return the refined version as plain text.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# User-provided idea\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# -- Step 1: Generate specs --\n",
        "task_specs = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for idea: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=specs_agent\n",
        ")\n",
        "crew_specs = Crew(\n",
        "    agents=[specs_agent],\n",
        "    tasks=[task_specs],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "result_specs = crew_specs.kickoff()\n",
        "\n",
        "specs_json = robust_json_extract(result_specs.tasks_output[0].raw)\n",
        "\n",
        "# -- Step 2: Generate chapter outline based on specs --\n",
        "task_outline = Task(\n",
        "    description=(\n",
        "        f\"Create a 10-item chapter outline as JSON array with objects having keys 'title' and 'summary' based on specs: {json.dumps(specs_json)}\"\n",
        "    ),\n",
        "    expected_output='json',\n",
        "    agent=prod_agent\n",
        ")\n",
        "crew_outline = Crew(\n",
        "    agents=[prod_agent],\n",
        "    tasks=[task_outline],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "result_outline = crew_outline.kickoff()\n",
        "\n",
        "outline_json = robust_json_extract(result_outline.tasks_output[0].raw)\n",
        "\n",
        "# Check for dictionary format and extract chapter list accordingly\n",
        "outline_list = outline_json[\"chapter\"] if isinstance(outline_json, dict) and \"chapter\" in outline_json else outline_json\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "\n",
        "# -- Step 3: Build per-chapter tasks: draft, evaluate, refine --\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Draft full chapter titled '{title}', aiming for approximately 2,000â€“4,000 words \"\n",
        "                \"to suit adult fiction pacing, based on specs and outline.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=prod_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=\"Evaluate the chapter draft for score and feedback as JSON.\",\n",
        "            expected_output='json',\n",
        "            agent=eval_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Polish and refine the prose of chapter '{title}', ensuring clarity, style consistency, \"\n",
        "                \"and adherence to tone and word-count guidelines.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=redac_agent\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Execute chapter tasks in batches to avoid rate limits\n",
        "batch_size = 3\n",
        "auto_results = []\n",
        "for i in range(0, len(chapter_tasks), batch_size):\n",
        "    batch = chapter_tasks[i:i+batch_size]\n",
        "    crew = Crew(\n",
        "        agents=[prod_agent, eval_agent, redac_agent],\n",
        "        tasks=batch,\n",
        "        process=Process.sequential,\n",
        "        verbose=True\n",
        "    )\n",
        "    time.sleep(1)\n",
        "    auto_results.extend(crew.kickoff())\n",
        "\n",
        "# -- Step 4: Display results --\n",
        "pprint.pprint({\n",
        "    'specs': specs_json,\n",
        "    'outline_titles': outline_titles,\n",
        "    'chapters_output': auto_results\n",
        "})\n"
      ],
      "metadata": {
        "id": "9nD5Q0RaIeNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnUn0UKLIeDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFNpf06PId_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouc7brXSId7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmF7wNUhHuTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes via Ollama\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice via Ollama\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "    json_str = match.group(1)\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "    json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "        return json.loads(json_str)\n",
        "\n",
        "# Initialize Agents with LiteLLM provider strings\n",
        "specs_agent = Agent(\n",
        "    role='SpecificationsAgent',\n",
        "    goal='Extract story specs (genre, theme, tone, word_count) as JSON',\n",
        "    backstory='Produce a JSON object with keys genre, theme, tone, word_count from a user idea.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "prod_agent = Agent(\n",
        "    role='ProductionAgent',\n",
        "    goal='Generate chapter content and implement creative changes via guidelines',\n",
        "    backstory='Generate full chapter text targeting 2000-4000 words based on outline and specs.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "eval_agent = Agent(\n",
        "    role='EvaluationAgent',\n",
        "    goal='Evaluate chapter drafts and return JSON with score and feedback',\n",
        "    backstory='Return JSON {\"score\":0-1, \"feedback\":\"...\"} evaluating chapter quality and adherence.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "redac_agent = Agent(\n",
        "    role='RedacteurAgent',\n",
        "    goal='Polish chapter prose and maintain consistency',\n",
        "    backstory='Take raw chapter text and return the refined version as plain text.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# User-provided idea\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# -- Step 1: Generate specs --\n",
        "task_specs = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for idea: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=specs_agent\n",
        ")\n",
        "crew_specs = Crew(\n",
        "    agents=[specs_agent],\n",
        "    tasks=[task_specs],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "result_specs = crew_specs.kickoff()\n",
        "\n",
        "specs_json = robust_json_extract(result_specs.tasks_output[0].raw)\n",
        "\n",
        "# -- Step 2: Generate chapter outline based on specs --\n",
        "task_outline = Task(\n",
        "    description=(\n",
        "        f\"Create a 10-item chapter outline as JSON array with objects having keys 'title' and 'summary' based on specs: {json.dumps(specs_json)}\"\n",
        "    ),\n",
        "    expected_output='json',\n",
        "    agent=prod_agent\n",
        ")\n",
        "crew_outline = Crew(\n",
        "    agents=[prod_agent],\n",
        "    tasks=[task_outline],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "result_outline = crew_outline.kickoff()\n",
        "\n",
        "outline_data = robust_json_extract(result_outline.tasks_output[0].raw)\n",
        "if isinstance(outline_data, dict) and 'chapter' in outline_data:\n",
        "    outline_list = outline_data['chapter']\n",
        "else:\n",
        "    outline_list = outline_data\n",
        "\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "# -- Step 3: Build per-chapter tasks: draft, evaluate, refine --\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Draft full chapter titled '{title}', aiming for approximately 2,000â€“4,000 words \"\n",
        "                \"to suit adult fiction pacing, based on specs and outline.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=prod_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=\"Evaluate the chapter draft for score and feedback as JSON.\",\n",
        "            expected_output='json',\n",
        "            agent=eval_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Polish and refine the prose of chapter '{title}', ensuring clarity, style consistency, \"\n",
        "                \"and adherence to tone and word-count guidelines.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=redac_agent\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Execute chapter tasks in batches to avoid rate limits\n",
        "batch_size = 3\n",
        "auto_results = []\n",
        "for i in range(0, len(chapter_tasks), batch_size):\n",
        "    batch = chapter_tasks[i:i+batch_size]\n",
        "    crew = Crew(\n",
        "        agents=[prod_agent, eval_agent, redac_agent],\n",
        "        tasks=batch,\n",
        "        process=Process.sequential,\n",
        "        verbose=True\n",
        "    )\n",
        "    time.sleep(1)\n",
        "    auto_results.extend(crew.kickoff())\n",
        "\n",
        "# -- Step 4: Save results as Word document --\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, level=1)\n",
        "\n",
        "# Optional formatting\n",
        "style = doc.styles['Normal']\n",
        "style.font.name = 'Times New Roman'\n",
        "style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
        "style.font.size = Pt(12)\n",
        "\n",
        "# Only use every 3rd item in auto_results, which are the refined chapters\n",
        "for chapter_dict, chapter_output in zip(outline_list, auto_results[2::3]):  # Redacted outputs only\n",
        "    doc.add_heading(chapter_dict['title'], level=2)\n",
        "    doc.add_paragraph(str(chapter_output).strip())\n",
        "\n",
        "\n",
        "output_file = 'generated_book.docx'\n",
        "doc.save(output_file)\n",
        "print(f\"âœ… Book generated and saved as '{output_file}'\")\n"
      ],
      "metadata": {
        "id": "GN-qPqNDHuPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYqGcEItaYzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AwMzjd2aYxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzdvK7_GaYwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmlsnqL5aYuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fG56SaGFaYsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes via Ollama\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice via Ollama\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "    json_str = match.group(1)\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "    json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "        return json.loads(json_str)\n",
        "\n",
        "# Initialize Agents\n",
        "specs_agent = Agent(\n",
        "    role='SpecificationsAgent',\n",
        "    goal='Extract story specs (genre, theme, tone, word_count) as JSON',\n",
        "    backstory='Produce a JSON object with keys genre, theme, tone, word_count from a user idea.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "prod_agent = Agent(\n",
        "    role='ProductionAgent',\n",
        "    goal='Generate chapter content and implement creative changes via guidelines',\n",
        "    backstory='Generate full chapter text targeting 2000-4000 words based on outline and specs.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "eval_agent = Agent(\n",
        "    role='EvaluationAgent',\n",
        "    goal='Evaluate chapter drafts and return JSON with score and feedback',\n",
        "    backstory='Return JSON {\"score\":0-1, \"feedback\":\"...\"} evaluating chapter quality and adherence.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "redac_agent = Agent(\n",
        "    role='RedacteurAgent',\n",
        "    goal='Polish chapter prose and maintain consistency',\n",
        "    backstory='Take raw chapter text and return the refined version as plain text.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# User-provided idea\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# Step 1: Generate specs\n",
        "task_specs = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for idea: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=specs_agent\n",
        ")\n",
        "crew_specs = Crew(\n",
        "    agents=[specs_agent],\n",
        "    tasks=[task_specs],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "result_specs = crew_specs.kickoff()\n",
        "specs_json = robust_json_extract(result_specs.tasks_output[0].raw)\n",
        "\n",
        "# Step 2: Generate chapter outline based on specs\n",
        "task_outline = Task(\n",
        "    description=(\n",
        "        f\"Create a 10-item chapter outline as JSON array with objects having keys 'title' and 'summary' based on specs: {json.dumps(specs_json)}\"\n",
        "    ),\n",
        "    expected_output='json',\n",
        "    agent=prod_agent\n",
        ")\n",
        "crew_outline = Crew(\n",
        "    agents=[prod_agent],\n",
        "    tasks=[task_outline],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "result_outline = crew_outline.kickoff()\n",
        "\n",
        "outline_data = robust_json_extract(result_outline.tasks_output[0].raw)\n",
        "outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', outline_data)\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "# Step 3: Build per-chapter tasks\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Draft full chapter titled '{title}', aiming for approximately 2,000â€“4,000 words \"\n",
        "                \"to suit adult fiction pacing, based on specs and outline.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=prod_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=\"Evaluate the chapter draft for score and feedback as JSON.\",\n",
        "            expected_output='json',\n",
        "            agent=eval_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Polish and refine the prose of chapter '{title}', ensuring clarity, style consistency, \"\n",
        "                \"and adherence to tone and word-count guidelines.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=redac_agent\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Step 4: Execute tasks in batches\n",
        "batch_size = 3\n",
        "auto_results = []\n",
        "for i in range(0, len(chapter_tasks), batch_size):\n",
        "    batch = chapter_tasks[i:i+batch_size]\n",
        "    crew = Crew(\n",
        "        agents=[prod_agent, eval_agent, redac_agent],\n",
        "        tasks=batch,\n",
        "        process=Process.sequential,\n",
        "        verbose=True\n",
        "    )\n",
        "    time.sleep(1)\n",
        "    auto_results.extend(crew.kickoff())\n",
        "\n",
        "# Step 5: Save to Word document\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, level=1)\n",
        "\n",
        "style = doc.styles['Normal']\n",
        "style.font.name = 'Times New Roman'\n",
        "style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
        "style.font.size = Pt(12)\n",
        "\n",
        "for chapter_dict, result in zip(outline_list, auto_results[2::3]):\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(chapter_dict['title'], level=2)\n",
        "    if isinstance(result, tuple):\n",
        "        result = result[1]  # unpack (task, result) tuple\n",
        "    content = getattr(result, 'raw', str(result))\n",
        "    content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL)\n",
        "    content = re.sub(r\"^\\(?\\\\'?raw\\\\'?,?\", \"\", content).strip()\n",
        "    doc.add_paragraph(content.strip())\n",
        "\n",
        "output_file = 'generated_book.docx'\n",
        "doc.save(output_file)\n",
        "print(f\"\\nðŸ“˜ Book successfully generated and saved as '{output_file}'\")\n"
      ],
      "metadata": {
        "id": "OaUJ4T3AaYkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGskKilNhdOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jqaj4XgrhdMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKk3VNJXhdJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27d19uv-kS68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXnwPMtTkS38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes via Ollama\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice via Ollama\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time, shutil, os\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from IPython.display import FileLink\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "    json_str = match.group(1)\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "    json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)  # Fix missing quotes\n",
        "        json_str = re.sub(r'\"([^\"]*?)\"\\s*:\\s*(\"[^\"]*?)?([,\\}])', r'\"\\1\": \\2\\3', json_str)  # Fix unescaped fields\n",
        "        return json.loads(json_str)\n",
        "\n",
        "\n",
        "# Initialize Agents\n",
        "specs_agent = Agent(\n",
        "    role='SpecificationsAgent',\n",
        "    goal='Extract story specs (genre, theme, tone, word_count) as JSON',\n",
        "    backstory='Produce a JSON object with keys genre, theme, tone, word_count from a user idea.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "prod_agent = Agent(\n",
        "    role='ProductionAgent',\n",
        "    goal='Generate chapter content and implement creative changes via guidelines',\n",
        "    backstory='Generate full chapter text targeting 2000-4000 words based on outline and specs.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "eval_agent = Agent(\n",
        "    role='EvaluationAgent',\n",
        "    goal='Evaluate chapter drafts and return JSON with score and feedback',\n",
        "    backstory='Return JSON {\"score\":0-1, \"feedback\":\"...\"} evaluating chapter quality and adherence.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "redac_agent = Agent(\n",
        "    role='RedacteurAgent',\n",
        "    goal='Polish chapter prose and maintain consistency',\n",
        "    backstory='Take raw chapter text and return the refined version as plain text.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# User-provided idea\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# Step 1: Generate specs\n",
        "task_specs = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for idea: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=specs_agent\n",
        ")\n",
        "crew_specs = Crew(\n",
        "    agents=[specs_agent],\n",
        "    tasks=[task_specs],\n",
        "    process=Process.sequential,\n",
        "    verbose=False\n",
        ")\n",
        "result_specs = crew_specs.kickoff()\n",
        "specs_json = robust_json_extract(result_specs.tasks_output[0].raw)\n",
        "\n",
        "# Step 2: Generate chapter outline based on specs\n",
        "task_outline = Task(\n",
        "    description=(\n",
        "        f\"Create a 10-item chapter outline as JSON array with objects having keys 'title' and 'summary' based on specs: {json.dumps(specs_json)}\"\n",
        "    ),\n",
        "    expected_output='json',\n",
        "    agent=prod_agent\n",
        ")\n",
        "crew_outline = Crew(\n",
        "    agents=[prod_agent],\n",
        "    tasks=[task_outline],\n",
        "    process=Process.sequential,\n",
        "    verbose=False\n",
        ")\n",
        "result_outline = crew_outline.kickoff()\n",
        "\n",
        "outline_data = robust_json_extract(result_outline.tasks_output[0].raw)\n",
        "outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', outline_data)\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "# Step 3: Build per-chapter tasks\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Draft full chapter titled '{title}', aiming for approximately 2,000â€“4,000 words \"\n",
        "                \"to suit adult fiction pacing, based on specs and outline.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=prod_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=\"Evaluate the chapter draft for score and feedback as JSON.\",\n",
        "            expected_output='json',\n",
        "            agent=eval_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Polish and refine the prose of chapter '{title}', ensuring clarity, style consistency, \"\n",
        "                \"and adherence to tone and word-count guidelines.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=redac_agent\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Step 4: Execute tasks in batches\n",
        "batch_size = 3\n",
        "auto_results = []\n",
        "for i in range(0, len(chapter_tasks), batch_size):\n",
        "    batch = chapter_tasks[i:i+batch_size]\n",
        "    crew = Crew(\n",
        "        agents=[prod_agent, eval_agent, redac_agent],\n",
        "        tasks=batch,\n",
        "        process=Process.sequential,\n",
        "        verbose=False\n",
        "    )\n",
        "    time.sleep(1)\n",
        "    auto_results.extend(crew.kickoff())\n",
        "\n",
        "# Step 5: Save to Word document\n",
        "\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, level=1)\n",
        "doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_start = doc._body._body[-1]\n",
        "toc_refs = []\n",
        "\n",
        "style = doc.styles['Normal']\n",
        "style.font.name = 'Times New Roman'\n",
        "style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
        "style.font.size = Pt(12)\n",
        "\n",
        "for i, (chapter_dict, result) in enumerate(zip(outline_list, auto_results[2::3]), start=1):\n",
        "    title = chapter_dict['title']\n",
        "    if isinstance(result, tuple):\n",
        "        result = result[1]  # unpack\n",
        "    content = getattr(result, 'raw', str(result))\n",
        "    content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL)\n",
        "    content = re.sub(r\"^[\\(\\{]?'?raw'?,?\", \"\", content).strip()\n",
        "    toc_refs.append(f\"Chapter {i}: {title}\")\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(title, level=2)\n",
        "    doc.add_paragraph(content.strip())\n",
        "\n",
        "# Insert Table of Contents manually\n",
        "for ref in toc_refs:\n",
        "    para = doc.paragraphs[1].insert_paragraph_before(ref)\n",
        "\n",
        "# Updated save path\n",
        "doc_path = '/content/sample_data/generated_book.docx'\n",
        "os.makedirs(os.path.dirname(doc_path), exist_ok=True)\n",
        "doc.save(doc_path)\n",
        "\n",
        "print(\"âœ… Book saved to:\", doc_path)\n",
        "FileLink(doc_path)\n"
      ],
      "metadata": {
        "id": "4w1FWtn3kS0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iOfyXAGZnAZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyWU1j9vnAWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6ufQ_TrnAUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xD7qap9wnAR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6tMcI3rpnAO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XwX1yfhnAMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes via Ollama\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice via Ollama\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time, shutil, os\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from IPython.display import FileLink\n",
        "\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    # Attempt to extract JSON object or array\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)  # remove trailing commas\n",
        "        json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)  # fix missing quotes\n",
        "            return json.loads(json_str)\n",
        "\n",
        "    # If no valid JSON found, fallback: attempt to parse bullet-style text\n",
        "    lines = raw_output.splitlines()\n",
        "    spec = {}\n",
        "    for line in lines:\n",
        "        if \"**\" in line and \":\" in line:\n",
        "            parts = re.findall(r'\\*\\*(.*?)\\*\\*:\\s*(.*)', line)\n",
        "            if parts:\n",
        "                key, value = parts[0]\n",
        "                spec[key.lower()] = value.strip().rstrip('.')\n",
        "    if spec:\n",
        "        # Clean known keys to match expected schema\n",
        "        return {\n",
        "            \"genre\": spec.get(\"genre\", \"\"),\n",
        "            \"theme\": spec.get(\"theme\", \"\"),\n",
        "            \"tone\": spec.get(\"tone\", \"\"),\n",
        "            \"word_count\": int(re.search(r'\\d+', spec.get(\"word count\", \"0\")).group())\n",
        "        }\n",
        "\n",
        "    raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "\n",
        "# Initialize Agents\n",
        "specs_agent = Agent(\n",
        "    role='SpecificationsAgent',\n",
        "    goal='Extract story specs (genre, theme, tone, word_count) as JSON',\n",
        "    backstory='Produce a JSON object with keys genre, theme, tone, word_count from a user idea.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "prod_agent = Agent(\n",
        "    role='ProductionAgent',\n",
        "    goal='Generate chapter content and implement creative changes via guidelines',\n",
        "    backstory='Generate full chapter text targeting 2000-4000 words based on outline and specs.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "eval_agent = Agent(\n",
        "    role='EvaluationAgent',\n",
        "    goal='Evaluate chapter drafts and return JSON with score and feedback',\n",
        "    backstory='Return JSON {\"score\":0-1, \"feedback\":\"...\"} evaluating chapter quality and adherence.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "redac_agent = Agent(\n",
        "    role='RedacteurAgent',\n",
        "    goal='Polish chapter prose and maintain consistency',\n",
        "    backstory='Take raw chapter text and return the refined version as plain text.',\n",
        "    llm='ollama/deepseek-r1:1.5b',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# User-provided idea\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# Step 1: Generate specs\n",
        "task_specs = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for idea: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=specs_agent\n",
        ")\n",
        "crew_specs = Crew(\n",
        "    agents=[specs_agent],\n",
        "    tasks=[task_specs],\n",
        "    process=Process.sequential,\n",
        "    verbose=False\n",
        ")\n",
        "result_specs = crew_specs.kickoff()\n",
        "specs_json = robust_json_extract(result_specs.tasks_output[0].raw)\n",
        "\n",
        "# Step 2: Generate chapter outline based on specs\n",
        "task_outline = Task(\n",
        "    description=(\n",
        "        f\"Create a 10-item chapter outline as JSON array with objects having keys 'title' and 'summary' based on specs: {json.dumps(specs_json)}\"\n",
        "    ),\n",
        "    expected_output='json',\n",
        "    agent=prod_agent\n",
        ")\n",
        "crew_outline = Crew(\n",
        "    agents=[prod_agent],\n",
        "    tasks=[task_outline],\n",
        "    process=Process.sequential,\n",
        "    verbose=False\n",
        ")\n",
        "result_outline = crew_outline.kickoff()\n",
        "\n",
        "outline_data = robust_json_extract(result_outline.tasks_output[0].raw)\n",
        "outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', outline_data)\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "# Step 3: Build per-chapter tasks\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Draft full chapter titled '{title}', aiming for approximately 2,000â€“4,000 words \"\n",
        "                \"to suit adult fiction pacing, based on specs and outline.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=prod_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=\"Evaluate the chapter draft for score and feedback as JSON.\",\n",
        "            expected_output='json',\n",
        "            agent=eval_agent\n",
        "        ),\n",
        "        Task(\n",
        "            description=(\n",
        "                f\"Polish and refine the prose of chapter '{title}', ensuring clarity, style consistency, \"\n",
        "                \"and adherence to tone and word-count guidelines.\"\n",
        "            ),\n",
        "            expected_output='text',\n",
        "            agent=redac_agent\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Step 4: Execute tasks in batches\n",
        "batch_size = 3\n",
        "auto_results = []\n",
        "for i in range(0, len(chapter_tasks), batch_size):\n",
        "    batch = chapter_tasks[i:i+batch_size]\n",
        "    crew = Crew(\n",
        "        agents=[prod_agent, eval_agent, redac_agent],\n",
        "        tasks=batch,\n",
        "        process=Process.sequential,\n",
        "        verbose=False\n",
        "    )\n",
        "    time.sleep(1)\n",
        "    auto_results.extend(crew.kickoff())\n",
        "\n",
        "# Step 5: Save to Word document\n",
        "\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# Save final cleaned-up book\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, level=1)\n",
        "\n",
        "doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_refs = []\n",
        "\n",
        "style = doc.styles['Normal']\n",
        "style.font.name = 'Times New Roman'\n",
        "style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
        "style.font.size = Pt(12)\n",
        "\n",
        "for i, (chapter_dict, result) in enumerate(zip(outline_list, auto_results[2::3]), start=1):\n",
        "    title = chapter_dict['title']\n",
        "\n",
        "    if isinstance(result, tuple):\n",
        "        result = result[1]\n",
        "    content = getattr(result, 'raw', str(result))\n",
        "\n",
        "    content = re.sub(r\"<think>.*?</think>\", \"\", content, flags=re.DOTALL)\n",
        "    content = re.sub(r\"(?i)TaskOutput\\(.*?raw=|expected_output=.*?\\)\", \"\", content, flags=re.DOTALL)\n",
        "    content = re.sub(r\"\\s*agent=['\\\"].+?['\\\"]\\s*\", \"\", content)\n",
        "    content = re.sub(r\"[{}\\[\\]()<>]\", \"\", content)\n",
        "    content = re.sub(r\"\\\\n|\\\\t|\\\\\", \"\\n\", content)\n",
        "    content = re.sub(r\"\\n{2,}\", \"\\n\\n\", content)\n",
        "    content = content.strip()\n",
        "\n",
        "    toc_refs.append(f\"Chapter {i}: {title}\")\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(f\"Chapter {i}: {title}\", level=2)\n",
        "    doc.add_paragraph(content)\n",
        "\n",
        "# Insert TOC at beginning\n",
        "toc_para = doc.paragraphs[1]\n",
        "for ref in toc_refs[::-1]:\n",
        "    toc_para.insert_paragraph_before(ref)\n",
        "\n",
        "# Save and display file\n",
        "output_path = '/content/sample_data/generated_book.docx'\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "doc.save(output_path)\n",
        "print(\"âœ… Book saved to:\", output_path)\n",
        "FileLink(output_path)\n"
      ],
      "metadata": {
        "id": "p0d_dn_anAJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-U0UmGfqiHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPUM8OyBqiCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlrgVdNSqh8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyKAIiOCucdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z36OxrHPucXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFPFOQatucUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeMsS3eTp26y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpKDgSmfp22j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXDXLZByp2y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice\n",
        "- ManagementAgent: Coordinates between agents and tracks creative flow\n",
        "- ChroniqueurAgent: Documents the creative journey\n",
        "- DocumentalisteAgent: Manages research and references\n",
        "- DuplicationAgent: Ensures originality and prevents redundancy\n",
        "- ValidationAgent: Ensures philosophical and ethical alignment\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx tqdm\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time, shutil, os\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from IPython.display import FileLink\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# JSON extractor\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_str = re.sub(r',\\s*([}}\\]])', r'\\1', json_str)\n",
        "        json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "            return json.loads(json_str)\n",
        "    lines = raw_output.splitlines()\n",
        "    spec = {}\n",
        "    for line in lines:\n",
        "        if \"**\" in line and \":\" in line:\n",
        "            parts = re.findall(r'\\*\\*(.*?)\\*\\*:\\s*(.*)', line)\n",
        "            if parts:\n",
        "                key, value = parts[0]\n",
        "                spec[key.lower()] = value.strip().rstrip('.')\n",
        "    if spec:\n",
        "        return {\n",
        "            \"genre\": spec.get(\"genre\", \"\"),\n",
        "            \"theme\": spec.get(\"theme\", \"\"),\n",
        "            \"tone\": spec.get(\"tone\", \"\"),\n",
        "            \"word_count\": int(re.search(r'\\d+', spec.get(\"word count\", \"0\")).group())\n",
        "        }\n",
        "    raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "\n",
        "# Agent factory\n",
        "\n",
        "def create_agent(role, goal, backstory):\n",
        "    return Agent(role=role, goal=goal, backstory=backstory, llm='ollama/deepseek-r1:1.5b', verbose=False)\n",
        "\n",
        "agents = {\n",
        "    \"SpecificationsAgent\": create_agent('SpecificationsAgent', 'Extract story specs (genre, theme, tone, word_count)', 'Produces specs as JSON.'),\n",
        "    \"ProductionAgent\": create_agent('ProductionAgent', 'Generate full chapter content creatively', 'Craft immersive and original story chapters.'),\n",
        "    \"EvaluationAgent\": create_agent('EvaluationAgent', 'Evaluate quality and theme alignment', 'Rates and gives constructive chapter feedback.'),\n",
        "    \"RedacteurAgent\": create_agent('RedacteurAgent', 'Refine chapter prose', 'Ensures clarity and consistent voice.'),\n",
        "    \"ManagementAgent\": create_agent('ManagementAgent', 'Orchestrate agents and ensure progress', 'Coordinates narrative efforts across all roles.'),\n",
        "    \"ChroniqueurAgent\": create_agent('ChroniqueurAgent', 'Document the creative journey', 'Keeps a log of decisions and developments.'),\n",
        "    \"DocumentalisteAgent\": create_agent('DocumentalisteAgent', 'Support narrative with research', 'Provides relevant contextual knowledge.'),\n",
        "    \"DuplicationAgent\": create_agent('DuplicationAgent', 'Prevent repetition and ensure novelty', 'Checks for originality and eliminates duplication.'),\n",
        "    \"ValidationAgent\": create_agent('ValidationAgent', 'Check philosophical and ethical integrity', 'Assures alignment with deeper themes.')\n",
        "}\n",
        "\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "print(\"ðŸ” Step 1: Generating story specifications...\")\n",
        "spec_task = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"SpecificationsAgent\"]\n",
        ")\n",
        "specs_result = Crew(agents=[agents[\"SpecificationsAgent\"]], tasks=[spec_task], process=Process.sequential).kickoff()\n",
        "specs_json = robust_json_extract(specs_result.tasks_output[0].raw)\n",
        "\n",
        "print(\"ðŸ§  Step 2: Generating chapter outline...\")\n",
        "outline_task = Task(\n",
        "    description=f\"Based on: {json.dumps(specs_json)}, generate a chapter outline (10-30 chapters) as JSON array with 'title' and 'summary'.\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"ProductionAgent\"]\n",
        ")\n",
        "outline_result = Crew(agents=[agents[\"ProductionAgent\"]], tasks=[outline_task], process=Process.sequential).kickoff()\n",
        "outline_data = robust_json_extract(outline_result.tasks_output[0].raw)\n",
        "outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', outline_data)\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "print(\"âœï¸ Step 3: Generating chapters...\")\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(description=f\"Draft chapter '{title}' based on outline and specs. Limit output to 500 words.\", expected_output='text', agent=agents[\"ProductionAgent\"]),\n",
        "        Task(description=f\"Evaluate draft of chapter '{title}'.\", expected_output='json', agent=agents[\"EvaluationAgent\"]),\n",
        "        Task(description=f\"Refine prose of chapter '{title}'.\", expected_output='text', agent=agents[\"RedacteurAgent\"]),\n",
        "    ])\n",
        "\n",
        "final_chapters = []\n",
        "for i in tqdm(range(0, len(chapter_tasks), 3), desc=\"ðŸ› ï¸ Generating Chapters\"):\n",
        "    crew = Crew(\n",
        "        agents=[agents[\"ProductionAgent\"], agents[\"EvaluationAgent\"], agents[\"RedacteurAgent\"]],\n",
        "        tasks=chapter_tasks[i:i+3],\n",
        "        process=Process.sequential\n",
        "    )\n",
        "    final_chapters.append(crew.kickoff())\n",
        "\n",
        "print(\"ðŸ“„ Step 4: Saving final book as DOCX with TOC...\")\n",
        "\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, 0)\n",
        "doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_paragraph = doc.paragraphs[1]\n",
        "toc_refs = []\n",
        "\n",
        "for idx, (chapter, result) in enumerate(zip(outline_list, final_chapters), 1):\n",
        "    chapter_title = chapter['title']\n",
        "    if isinstance(result, tuple) and len(result) >= 3:\n",
        "        refined = result[-1]\n",
        "    elif isinstance(result, list) and len(result) >= 3:\n",
        "        refined = result[-1]\n",
        "    else:\n",
        "        refined = result\n",
        "    chapter_text = getattr(refined, 'raw', str(refined))\n",
        "    chapter_text = re.sub(r\"<think>.*?</think>\", \"\", chapter_text, flags=re.DOTALL).strip()\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(f\"Chapter {idx}: {chapter_title}\", level=2)\n",
        "    doc.add_paragraph(\"[Generated image placeholder]\\n\")\n",
        "    doc.add_paragraph(chapter_text)\n",
        "    toc_refs.append(f\"Chapter {idx}: {chapter_title}\")\n",
        "\n",
        "for ref in reversed(toc_refs):\n",
        "    toc_paragraph.insert_paragraph_before(ref)\n",
        "\n",
        "output_path = '/content/sample_data/generated_book.docx'\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "doc.save(output_path)\n",
        "print(\"âœ… Book saved to:\", output_path)\n",
        "FileLink(output_path)\n"
      ],
      "metadata": {
        "id": "egwJbuwyp2vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dM5POyyssY-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYRkUx85sY6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bX0t8mnxsY3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "#generates something\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice\n",
        "- ManagementAgent: Coordinates between agents and tracks creative flow\n",
        "- ChroniqueurAgent: Documents the creative journey\n",
        "- DocumentalisteAgent: Manages research and references\n",
        "- DuplicationAgent: Ensures originality and prevents redundancy\n",
        "- ValidationAgent: Ensures philosophical and ethical alignment\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx tqdm\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, pprint, re, time, shutil, os\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from IPython.display import FileLink\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# JSON extractor\n",
        "\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_str = re.sub(r',\\s*([}}\\]])', r'\\1', json_str)\n",
        "        json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "            return json.loads(json_str)\n",
        "    lines = raw_output.splitlines()\n",
        "    spec = {}\n",
        "    for line in lines:\n",
        "        if \"**\" in line and \":\" in line:\n",
        "            parts = re.findall(r'\\*\\*(.*?)\\*\\*:\\s*(.*)', line)\n",
        "            if parts:\n",
        "                key, value = parts[0]\n",
        "                spec[key.lower()] = value.strip().rstrip('.')\n",
        "    if spec:\n",
        "        return {\n",
        "            \"genre\": spec.get(\"genre\", \"\"),\n",
        "            \"theme\": spec.get(\"theme\", \"\"),\n",
        "            \"tone\": spec.get(\"tone\", \"\"),\n",
        "            \"word_count\": int(re.search(r'\\d+', spec.get(\"word count\", \"0\")).group())\n",
        "        }\n",
        "    raise ValueError(f\"Failed to find JSON. Raw output was:\\n{raw_output}\")\n",
        "\n",
        "# Agent factory\n",
        "\n",
        "def create_agent(role, goal, backstory):\n",
        "    return Agent(role=role, goal=goal, backstory=backstory, llm='ollama/deepseek-r1:1.5b', verbose=False)\n",
        "\n",
        "agents = {\n",
        "    \"SpecificationsAgent\": create_agent('SpecificationsAgent', 'Extract story specs (genre, theme, tone, word_count)', 'Produces specs as JSON.'),\n",
        "    \"ProductionAgent\": create_agent('ProductionAgent', 'Generate full chapter content creatively', 'Craft immersive and original story chapters.'),\n",
        "    \"EvaluationAgent\": create_agent('EvaluationAgent', 'Evaluate quality and theme alignment', 'Rates and gives constructive chapter feedback.'),\n",
        "    \"RedacteurAgent\": create_agent('RedacteurAgent', 'Refine chapter prose', 'Ensures clarity and consistent voice.'),\n",
        "    \"ManagementAgent\": create_agent('ManagementAgent', 'Orchestrate agents and ensure progress', 'Coordinates narrative efforts across all roles.'),\n",
        "    \"ChroniqueurAgent\": create_agent('ChroniqueurAgent', 'Document the creative journey', 'Keeps a log of decisions and developments.'),\n",
        "    \"DocumentalisteAgent\": create_agent('DocumentalisteAgent', 'Support narrative with research', 'Provides relevant contextual knowledge.'),\n",
        "    \"DuplicationAgent\": create_agent('DuplicationAgent', 'Prevent repetition and ensure novelty', 'Checks for originality and eliminates duplication.'),\n",
        "    \"ValidationAgent\": create_agent('ValidationAgent', 'Check philosophical and ethical integrity', 'Assures alignment with deeper themes.')\n",
        "}\n",
        "\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "print(\"ðŸ” Step 1: Generating story specifications...\")\n",
        "spec_task = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"SpecificationsAgent\"]\n",
        ")\n",
        "specs_result = Crew(agents=[agents[\"SpecificationsAgent\"]], tasks=[spec_task], process=Process.sequential).kickoff()\n",
        "specs_json = robust_json_extract(specs_result.tasks_output[0].raw)\n",
        "\n",
        "print(\"ðŸ§  Step 2: Generating chapter outline...\")\n",
        "outline_task = Task(\n",
        "    description=f\"Based on: {json.dumps(specs_json)}, generate a chapter outline (10-30 chapters) as JSON array with 'title' and 'summary'.\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"ProductionAgent\"]\n",
        ")\n",
        "outline_result = Crew(agents=[agents[\"ProductionAgent\"]], tasks=[outline_task], process=Process.sequential).kickoff()\n",
        "outline_data = robust_json_extract(outline_result.tasks_output[0].raw)\n",
        "outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', outline_data)\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "print(\"âœï¸ Step 3: Generating chapters...\")\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(description=f\"Draft chapter '{title}' based on outline and specs. Limit output to 500 words.\", expected_output='text', agent=agents[\"ProductionAgent\"]),\n",
        "        Task(description=f\"Evaluate draft of chapter '{title}'.\", expected_output='json', agent=agents[\"EvaluationAgent\"]),\n",
        "        Task(description=f\"Refine prose of chapter '{title}'.\", expected_output='text', agent=agents[\"RedacteurAgent\"]),\n",
        "    ])\n",
        "\n",
        "final_chapters = []\n",
        "for i in tqdm(range(0, len(chapter_tasks), 3), desc=\"ðŸ› ï¸ Generating Chapters\"):\n",
        "    crew = Crew(\n",
        "        agents=[agents[\"ProductionAgent\"], agents[\"EvaluationAgent\"], agents[\"RedacteurAgent\"]],\n",
        "        tasks=chapter_tasks[i:i+3],\n",
        "        process=Process.sequential\n",
        "    )\n",
        "    try:\n",
        "        result = crew.kickoff()\n",
        "        final_chapters.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error in chapter generation: {e}\")\n",
        "        final_chapters.append(\"Chapter generation failed.\")\n",
        "\n",
        "print(\"ðŸ“„ Step 4: Saving final book as DOCX with TOC...\")\n",
        "\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, 0)\n",
        "doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_paragraph = doc.paragraphs[1]\n",
        "toc_refs = []\n",
        "\n",
        "for idx, (chapter, result) in enumerate(zip(outline_list, final_chapters), 1):\n",
        "    chapter_title = chapter['title']\n",
        "    try:\n",
        "        refined = result[-1] if isinstance(result, (list, tuple)) else result\n",
        "        chapter_text = getattr(refined, 'raw', str(refined))\n",
        "        chapter_text = re.sub(r\"<think>.*?</think>\", \"\", chapter_text, flags=re.DOTALL).strip()\n",
        "    except Exception:\n",
        "        chapter_text = \"[Error generating chapter content.]\"\n",
        "\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(f\"Chapter {idx}: {chapter_title}\", level=2)\n",
        "    doc.add_paragraph(\"[Generated image placeholder]\\n\")\n",
        "    doc.add_paragraph(chapter_text)\n",
        "    toc_refs.append(f\"Chapter {idx}: {chapter_title}\")\n",
        "\n",
        "for ref in reversed(toc_refs):\n",
        "    toc_paragraph.insert_paragraph_before(ref)\n",
        "\n",
        "output_path = '/content/sample_data/generated_book.docx'\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "doc.save(output_path)\n",
        "print(\"âœ… Book saved to:\", output_path)\n",
        "FileLink(output_path)\n"
      ],
      "metadata": {
        "id": "j6-9sCuUsYz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCq7u92gu5-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5lB3Mxdu57F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBu-Sia7u54Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gu_hmD-Hu50-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FMFoFuw9u5xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Uvc-y1Fxulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3L8vfFH_xuhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_gA0xxr14m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKRW8YSi14jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3z5OCtv14gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-WKRRK814dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chatter removed.. get  actual story"
      ],
      "metadata": {
        "id": "1gWgFIUkxucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama's deepseek-r1:1.5b model for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice\n",
        "- ManagementAgent: Coordinates between agents and tracks creative flow\n",
        "- ChroniqueurAgent: Documents the creative journey\n",
        "- DocumentalisteAgent: Manages research and references\n",
        "- DuplicationAgent: Ensures originality and prevents redundancy\n",
        "- ValidationAgent: Ensures philosophical and ethical alignment\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx tqdm\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, re, os\n",
        "from docx import Document\n",
        "from IPython.display import FileLink\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Utility: JSON cleaner ---\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "        json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "            return json.loads(json_str)\n",
        "    raise ValueError(\"Could not extract JSON from model output\")\n",
        "\n",
        "# --- Agent Factory ---\n",
        "def create_agent(role, goal, backstory):\n",
        "    return Agent(role=role, goal=goal, backstory=backstory, llm='ollama/deepseek-r1:1.5b', verbose=False)\n",
        "\n",
        "# --- Agent Definitions ---\n",
        "agents = {\n",
        "    \"SpecificationsAgent\": create_agent('SpecificationsAgent', 'Extract story specs (genre, theme, tone, word_count)', 'Produces specs as JSON.'),\n",
        "    \"ProductionAgent\": create_agent('ProductionAgent', 'Generate full chapter content creatively', 'Craft immersive and original story chapters.'),\n",
        "    \"EvaluationAgent\": create_agent('EvaluationAgent', 'Evaluate quality and theme alignment', 'Rates and gives constructive chapter feedback.'),\n",
        "    \"RedacteurAgent\": create_agent('RedacteurAgent', 'Refine chapter prose', 'Ensures clarity and consistent voice.')\n",
        "}\n",
        "\n",
        "# --- User Input ---\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# --- Step 1: Get Story Specifications ---\n",
        "print(\"ðŸ” Step 1: Extracting story specs...\")\n",
        "spec_task = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"SpecificationsAgent\"]\n",
        ")\n",
        "spec_crew = Crew(\n",
        "    agents=[agents[\"SpecificationsAgent\"]],\n",
        "    tasks=[spec_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "spec_result = spec_crew.kickoff()\n",
        "specs = robust_json_extract(spec_result.tasks_output[0].raw)\n",
        "\n",
        "# --- Step 2: Chapter Outline ---\n",
        "print(\"ðŸ§  Step 2: Generating outline...\")\n",
        "outline_task = Task(\n",
        "    description=f\"Generate a 10-30 chapter outline for story with specs: {json.dumps(specs)}. Output JSON list of objects with 'title' and 'summary'.\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"ProductionAgent\"]\n",
        ")\n",
        "outline_crew = Crew(\n",
        "    agents=[agents[\"ProductionAgent\"]],\n",
        "    tasks=[outline_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "outline_result = outline_crew.kickoff()\n",
        "outline_data = robust_json_extract(outline_result.tasks_output[0].raw)\n",
        "outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', list(outline_data.values()))\n",
        "outline_titles = [c['title'] for c in outline_list]\n",
        "\n",
        "# --- Step 3: Generate Chapters ---\n",
        "print(\"âœï¸ Step 3: Generating chapters...\")\n",
        "chapter_tasks = []\n",
        "for title in outline_titles:\n",
        "    chapter_tasks.extend([\n",
        "        Task(description=f\"Draft chapter '{title}' (max 500 words) with strong narrative flow. No agent chatter.\", expected_output='text', agent=agents[\"ProductionAgent\"]),\n",
        "        Task(description=f\"Evaluate chapter '{title}' quality.\", expected_output='text', agent=agents[\"EvaluationAgent\"]),\n",
        "        Task(description=f\"Refine chapter '{title}' to be immersive, clean, and in-character.\", expected_output='text', agent=agents[\"RedacteurAgent\"])\n",
        "    ])\n",
        "\n",
        "final_chapters = []\n",
        "for i in tqdm(range(0, len(chapter_tasks), 3), desc=\"ðŸ“š Generating Chapters\"):\n",
        "    crew = Crew(\n",
        "        agents=[agents[\"ProductionAgent\"], agents[\"EvaluationAgent\"], agents[\"RedacteurAgent\"]],\n",
        "        tasks=chapter_tasks[i:i+3],\n",
        "        process=Process.sequential\n",
        "    )\n",
        "    try:\n",
        "        result = crew.kickoff()\n",
        "        final_chapters.append(result[-1])  # Only take refined\n",
        "    except Exception as e:\n",
        "        final_chapters.append(f\"[Chapter generation failed: {e}]\")\n",
        "\n",
        "# --- Step 4: Save as .docx with TOC and placeholder images ---\n",
        "print(\"ðŸ“„ Step 4: Saving to Word document...\")\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, 0)\n",
        "toc = doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_refs = []\n",
        "\n",
        "for idx, (chapter, content) in enumerate(zip(outline_list, final_chapters), 1):\n",
        "    doc.add_page_break()\n",
        "    title = chapter['title']\n",
        "    toc_refs.append(f\"Chapter {idx}: {title}\")\n",
        "    doc.add_heading(f\"Chapter {idx}: {title}\", level=2)\n",
        "    doc.add_paragraph(\"[Generated image placeholder]\\n\")\n",
        "    text = getattr(content, 'raw', str(content))\n",
        "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "    cleaned = re.sub(r'(\\{.*\\}|\\[.*\\])', '', cleaned, flags=re.DOTALL)\n",
        "    doc.add_paragraph(cleaned)\n",
        "\n",
        "for ref in reversed(toc_refs):\n",
        "    toc.insert_paragraph_before(ref)\n",
        "\n",
        "output_path = \"/content/sample_data/generated_book.docx\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "doc.save(output_path)\n",
        "print(\"âœ… Book saved to:\", output_path)\n",
        "FileLink(output_path)\n"
      ],
      "metadata": {
        "id": "2EAbAXri21L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzRlt39rMYXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJtX-1-lMYSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxrGoTKqZ32q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuaAhh45cKPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlOpg083cKMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXsbDEnccKI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hkpdDI45cKFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tO6daarTjjJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNvTqgbkjjGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8liQYixjjDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o26uyuKNjjAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgnVUBhKji84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9oG7YCAji5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and OpenAI/Ollama fallback\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama (or OpenAI fallback) for content generation.\n",
        "Agents:\n",
        "- SpecificationsAgent: Analyzes story requirements and maintains narrative consistency\n",
        "- ProductionAgent: Generates content and implements creative changes\n",
        "- EvaluationAgent: Reviews quality and thematic resonance\n",
        "- RedacteurAgent: Refines prose and maintains voice\n",
        "\"\"\"\n",
        "\n",
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install crewai python-docx tqdm openai langchain_ollama\n",
        "\n",
        "# Imports\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, re, os\n",
        "from docx import Document\n",
        "from IPython.display import FileLink\n",
        "from tqdm.notebook import tqdm\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# --- LLM Fallback ---\n",
        "def get_llm():\n",
        "    try:\n",
        "        return OllamaLLM(model=\"ollama/deepseek-r1:1.5b\")\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Ollama failed, using GPT-4 fallback.\")\n",
        "        return ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "llm_fallback = get_llm()\n",
        "\n",
        "# --- Utility: JSON cleaner ---\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "        json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "            return json.loads(json_str)\n",
        "    raise ValueError(\"Could not extract JSON from model output\")\n",
        "\n",
        "# --- Agent Factory ---\n",
        "def create_agent(role, goal, backstory):\n",
        "    return Agent(role=role, goal=goal, backstory=backstory, llm=llm_fallback, verbose=False)\n",
        "\n",
        "# --- Agent Definitions ---\n",
        "agents = {\n",
        "    \"SpecificationsAgent\": create_agent('SpecificationsAgent', 'Extract story specs (genre, theme, tone, word_count)', 'Produces specs as JSON.'),\n",
        "    \"ProductionAgent\": create_agent('ProductionAgent', 'Generate full chapter content creatively', 'Craft immersive and original story chapters.'),\n",
        "    \"EvaluationAgent\": create_agent('EvaluationAgent', 'Evaluate quality and theme alignment', 'Rates and gives constructive chapter feedback.'),\n",
        "    \"RedacteurAgent\": create_agent('RedacteurAgent', 'Refine chapter prose', 'Ensures clarity and consistent voice.')\n",
        "}\n",
        "\n",
        "# --- User Input ---\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# --- Step 1: Get Story Specifications ---\n",
        "print(\"ðŸ” Step 1: Extracting story specs...\")\n",
        "spec_task = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"SpecificationsAgent\"]\n",
        ")\n",
        "spec_crew = Crew(\n",
        "    agents=[agents[\"SpecificationsAgent\"]],\n",
        "    tasks=[spec_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "try:\n",
        "    spec_result = spec_crew.kickoff()\n",
        "    specs = robust_json_extract(spec_result.tasks_output[0].raw)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"âŒ Failed to extract specs: {e}\")\n",
        "\n",
        "# --- Step 2: Chapter Outline ---\n",
        "print(\"ðŸ§  Step 2: Generating outline...\")\n",
        "outline_task = Task(\n",
        "    description=f\"Generate a 2-3 chapter outline for story with specs: {json.dumps(specs)}. Output JSON list of objects with 'title' and 'summary'.\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"ProductionAgent\"]\n",
        ")\n",
        "outline_crew = Crew(\n",
        "    agents=[agents[\"ProductionAgent\"]],\n",
        "    tasks=[outline_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "try:\n",
        "    outline_result = outline_crew.kickoff()\n",
        "    outline_data = robust_json_extract(outline_result.tasks_output[0].raw)\n",
        "    outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', list(outline_data.values()))\n",
        "    outline_titles = [c['title'] for c in outline_list]\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"âŒ Failed to generate outline: {e}\")\n",
        "\n",
        "# --- Step 3: Generate Chapters ---\n",
        "print(\"âœï¸ Step 3: Generating chapters...\")\n",
        "chapter_tasks = []\n",
        "for chapter in outline_list:\n",
        "    title = chapter['title']\n",
        "    prompt = f\"Write a fully detailed story chapter titled '{title}' (400-600 words). Stay fully in-story. No commentary, no JSON.\"\n",
        "    chapter_tasks.extend([\n",
        "        Task(description=prompt, expected_output='text', agent=agents[\"ProductionAgent\"]),\n",
        "        Task(description=f\"Evaluate chapter '{title}' quality only internally.\", expected_output='text', agent=agents[\"EvaluationAgent\"]),\n",
        "        Task(description=f\"Refine chapter '{title}' and return only the narrative.\", expected_output='text', agent=agents[\"RedacteurAgent\"])\n",
        "    ])\n",
        "\n",
        "final_chapters = []\n",
        "for i in tqdm(range(0, len(chapter_tasks), 3), desc=\"ðŸ“š Generating Chapters\"):\n",
        "    crew = Crew(\n",
        "        agents=[agents[\"ProductionAgent\"], agents[\"EvaluationAgent\"], agents[\"RedacteurAgent\"]],\n",
        "        tasks=chapter_tasks[i:i+3],\n",
        "        process=Process.sequential\n",
        "    )\n",
        "    try:\n",
        "        result = crew.kickoff()\n",
        "        final_chapters.append(result[-1])  # Only take refined\n",
        "    except Exception as e:\n",
        "        final_chapters.append(f\"[Chapter generation failed: {e}]\")\n",
        "\n",
        "# --- Step 4: Save as .docx with TOC and placeholder images ---\n",
        "print(\"ðŸ“„ Step 4: Saving to Word document...\")\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, 0)\n",
        "toc = doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_refs = []\n",
        "\n",
        "for idx, (chapter, content) in enumerate(zip(outline_list, final_chapters), 1):\n",
        "    doc.add_page_break()\n",
        "    title = chapter['title']\n",
        "    toc_refs.append(f\"Chapter {idx}: {title}\")\n",
        "    doc.add_heading(f\"Chapter {idx}: {title}\", level=2)\n",
        "    doc.add_paragraph(\"[Generated image placeholder]\\n\")\n",
        "    text = getattr(content, 'raw', str(content))\n",
        "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "    cleaned = re.sub(r'(\\{.*\\}|\\[.*\\])', '', cleaned, flags=re.DOTALL)\n",
        "    doc.add_paragraph(cleaned)\n",
        "\n",
        "for ref in reversed(toc_refs):\n",
        "    toc.insert_paragraph_before(ref)\n",
        "\n",
        "output_path = \"/content/sample_data/generated_book.docx\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "doc.save(output_path)\n",
        "print(\"âœ… Book saved to:\", output_path)\n",
        "FileLink(output_path)\n"
      ],
      "metadata": {
        "id": "KNALoiudji1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ij7T6AoroLMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNbwZlgLyvxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbTuEsivyvu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Agent Book Generation with CrewAI and Ollama\n",
        "\n",
        "\"\"\"\n",
        "This notebook demonstrates how to set up a multi-agent system in CrewAI\n",
        "that leverages Ollama (using langchain_ollama.OllamaLLM, no LiteLLM dependency)\n",
        "for content generation.\n",
        "\"\"\"\n",
        "\n",
        "# --- Setup Ollama Server for Colab ---\n",
        "import os, threading, subprocess, time, requests\n",
        "\n",
        "# Disable LiteLLM entirely\n",
        "os.environ[\"LITELLM_USE_ENV\"] = \"False\"\n",
        "os.environ[\"CREWAI_USE_LITELLM\"] = \"False\"\n",
        "\n",
        "# Clear conflicting LiteLLM/OpenAI config\n",
        "os.environ.pop(\"OPENAI_API_KEY\", None)\n",
        "os.environ.pop(\"LITELLM_PROVIDER\", None)\n",
        "os.environ.pop(\"LITELLM_MODEL\", None)\n",
        "os.environ.pop(\"LITELLM_BASE_URL\", None)\n",
        "\n",
        "# Allow Ollama origin\n",
        "os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n",
        "os.environ[\"OLLAMA_ORIGINS\"] = \"*\"\n",
        "\n",
        "def _serve_ollama():\n",
        "    try:\n",
        "        subprocess.Popen([\"ollama\", \"serve\"])\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Could not start Ollama:\", e)\n",
        "\n",
        "threading.Thread(target=_serve_ollama, daemon=True).start()\n",
        "time.sleep(10)\n",
        "\n",
        "try:\n",
        "    r = requests.get(\"http://127.0.0.1:11434\")\n",
        "    print(\"âœ… Ollama server running:\", r.status_code)\n",
        "    !ollama pull deepseek-r1:1.5b\n",
        "except:\n",
        "    raise RuntimeError(\"âŒ Ollama server not responding â€” check install and port\")\n",
        "\n",
        "# --- Dependencies ---\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import json, re\n",
        "from docx import Document\n",
        "from IPython.display import FileLink\n",
        "from tqdm.notebook import tqdm\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Shared Ollama LLM Instance ---\n",
        "ollama_llm = OllamaLLM(\n",
        "    model=\"deepseek-r1:1.5b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# --- Patch Agent to use Ollama forcibly ---\n",
        "from crewai.agent import Agent as CrewAgent\n",
        "from types import MethodType\n",
        "\n",
        "def patched_run_task(self, task):\n",
        "    response = ollama_llm.invoke(task.description)\n",
        "    class Dummy:\n",
        "        raw = response\n",
        "    return Dummy()\n",
        "\n",
        "CrewAgent._run_task = MethodType(patched_run_task, CrewAgent)\n",
        "\n",
        "# --- Patch Crew to avoid LiteLLM ---\n",
        "from crewai.crew import Crew as OriginalCrew\n",
        "\n",
        "from crewai.task import TaskOutput\n",
        "\n",
        "from crewai.task import TaskOutput\n",
        "\n",
        "def patched_kickoff(self):\n",
        "    outputs = []\n",
        "    for task in self.tasks:\n",
        "        agent = task.agent\n",
        "        result = agent._run_task(task)\n",
        "        outputs.append(TaskOutput(\n",
        "            raw=getattr(result, 'raw', str(result)),\n",
        "            output=str(result),\n",
        "            description=task.description,\n",
        "            agent=agent.role if hasattr(agent, 'role') else str(agent)\n",
        "        ))\n",
        "    return outputs\n",
        "\n",
        "OriginalCrew.kickoff = patched_kickoff\n",
        "\n",
        "# --- Utility: JSON cleaner ---\n",
        "def robust_json_extract(raw_output):\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}|\\[.*?\\])\\s*```', raw_output, re.DOTALL)\n",
        "    if not match:\n",
        "        match = re.search(r'(\\{.*?\\}|\\[.*?\\])', raw_output, re.DOTALL)\n",
        "    if match:\n",
        "        json_str = match.group(1)\n",
        "        json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "        json_str = json_str.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\', '\\\\\\\\')\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            json_str = re.sub(r'(\\w+)\\s*:', r'\"\\1\":', json_str)\n",
        "            return json.loads(json_str)\n",
        "    raise ValueError(\"Could not extract JSON from model output\")\n",
        "\n",
        "# --- Agent Factory ---\n",
        "def create_agent(role, goal, backstory):\n",
        "    return Agent(\n",
        "        role=role,\n",
        "        goal=goal,\n",
        "        backstory=backstory,\n",
        "        llm=ollama_llm,\n",
        "        verbose=True,\n",
        "        allow_delegation=False,\n",
        "        use_litellm=False\n",
        "    )\n",
        "\n",
        "# --- Agent Definitions ---\n",
        "agents = {\n",
        "    \"SpecificationsAgent\": create_agent('SpecificationsAgent', 'Extract story specs (genre, theme, tone, word_count)', 'Produces specs as JSON.'),\n",
        "    \"ProductionAgent\": create_agent('ProductionAgent', 'Generate full chapter content creatively', 'Craft immersive and original story chapters. Only return the final story text.'),\n",
        "    \"EvaluationAgent\": create_agent('EvaluationAgent', 'Evaluate quality and theme alignment', 'Rates and gives constructive chapter feedback.'),\n",
        "    \"RedacteurAgent\": create_agent('RedacteurAgent', 'Refine chapter prose', 'Ensures clarity and consistent voice.')\n",
        "}\n",
        "\n",
        "# --- User Input ---\n",
        "initial_idea = \"A young scholar discovers an ancient prophecy that could save the kingdom.\"\n",
        "\n",
        "# --- Step 1: Get Story Specifications ---\n",
        "print(\"ðŸ” Step 1: Extracting story specs...\")\n",
        "spec_task = Task(\n",
        "    description=f\"Extract genre, theme, tone, and word_count as JSON for: '{initial_idea}'\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"SpecificationsAgent\"]\n",
        ")\n",
        "spec_crew = Crew(\n",
        "    agents=[agents[\"SpecificationsAgent\"]],\n",
        "    tasks=[spec_task],\n",
        "    process=Process.sequential,\n",
        "    use_litellm=False,\n",
        "    llm_config={\"llm\": ollama_llm}\n",
        ")\n",
        "try:\n",
        "    spec_result = spec_crew.kickoff()\n",
        "    specs = robust_json_extract(spec_result[0].raw)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"âŒ Failed to extract specs: {e}\")\n",
        "\n",
        "# --- Step 2: Chapter Outline ---\n",
        "print(\"ðŸ§  Step 2: Generating outline...\")\n",
        "outline_task = Task(\n",
        "    description=f\"Generate a 2-3 chapter outline for story with specs: {json.dumps(specs)}. Output JSON list of objects with 'title' and 'summary'.\",\n",
        "    expected_output='json',\n",
        "    agent=agents[\"ProductionAgent\"]\n",
        ")\n",
        "outline_crew = Crew(\n",
        "    agents=[agents[\"ProductionAgent\"]],\n",
        "    tasks=[outline_task],\n",
        "    process=Process.sequential,\n",
        "    use_litellm=False,\n",
        "    llm_config={\"llm\": ollama_llm}\n",
        ")\n",
        "try:\n",
        "    outline_result = outline_crew.kickoff()\n",
        "    outline_data = robust_json_extract(outline_result[0].raw)\n",
        "    outline_list = outline_data if isinstance(outline_data, list) else outline_data.get('chapter', list(outline_data.values()))\n",
        "    outline_titles = [c['title'] for c in outline_list]\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"âŒ Failed to generate outline: {e}\")\n",
        "\n",
        "# --- Step 3: Generate Chapters ---\n",
        "print(\"âœï¸ Step 3: Generating chapters...\")\n",
        "chapter_tasks = []\n",
        "for chapter in outline_list:\n",
        "    title = chapter['title']\n",
        "    prompt = f\"Write a full in-story chapter titled '{title}' (400â€“600 words). Only return the story text. No formatting or commentary.\"\n",
        "    chapter_tasks.extend([\n",
        "        Task(description=prompt, expected_output='text', agent=agents[\"ProductionAgent\"]),\n",
        "        Task(description=f\"Evaluate chapter '{title}' internally only.\", expected_output='text', agent=agents[\"EvaluationAgent\"]),\n",
        "        Task(description=f\"Refine prose for chapter '{title}'. Only return story.\", expected_output='text', agent=agents[\"RedacteurAgent\"])\n",
        "    ])\n",
        "\n",
        "final_chapters = []\n",
        "for i in tqdm(range(0, len(chapter_tasks), 3), desc=\"ðŸ“š Generating Chapters\"):\n",
        "    crew = Crew(\n",
        "        agents=[agents[\"ProductionAgent\"], agents[\"EvaluationAgent\"], agents[\"RedacteurAgent\"]],\n",
        "        tasks=chapter_tasks[i:i+3],\n",
        "        process=Process.sequential,\n",
        "        use_litellm=False,\n",
        "        llm_config={\"llm\": ollama_llm}\n",
        "    )\n",
        "    try:\n",
        "        result = crew.kickoff()\n",
        "        final_chapters.append(result[-1])\n",
        "    except Exception as e:\n",
        "        final_chapters.append(f\"[Chapter generation failed: {e}]\")\n",
        "\n",
        "# --- Step 4: Save as .docx with TOC and image stubs ---\n",
        "print(\"ðŸ“„ Step 4: Saving to Word document...\")\n",
        "doc = Document()\n",
        "doc.add_heading(initial_idea, 0)\n",
        "toc = doc.add_paragraph(\"\\nTable of Contents:\\n\")\n",
        "toc_refs = []\n",
        "\n",
        "for idx, (chapter, content) in enumerate(zip(outline_list, final_chapters), 1):\n",
        "    doc.add_page_break()\n",
        "    title = chapter['title']\n",
        "    toc_refs.append(f\"Chapter {idx}: {title}\")\n",
        "    doc.add_heading(f\"Chapter {idx}: {title}\", level=2)\n",
        "    doc.add_paragraph(\"[Generated image placeholder]\\n\")\n",
        "    text = getattr(content, 'raw', str(content))\n",
        "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "    cleaned = re.sub(r'(\\{.*\\}|\\[.*\\])', '', cleaned, flags=re.DOTALL)\n",
        "    doc.add_paragraph(cleaned)\n",
        "\n",
        "for ref in reversed(toc_refs):\n",
        "    toc.insert_paragraph_before(ref)\n",
        "\n",
        "output_path = \"/content/sample_data/generated_book.docx\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "doc.save(output_path)\n",
        "print(\"âœ… Book saved to:\", output_path)\n",
        "FileLink(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "25aac677146e483882d2b256cee3b384",
            "2a708ec811054dc8927e93f0816ea96a",
            "4cdf667b6f104f1788fea46b66bd5236",
            "2126e3d69d1143dc81c33c5ffe96f427",
            "e89c4bcbbb8c4e9782f46c41cfe5cc6f",
            "dbe090794106437491226c9911b26f9c",
            "beeee5eeb1684419836947fe0b9ce30b",
            "ab3b263bb76c4961a28ba8e0373089f5",
            "c49d2de01c694393b429b39072422d4e",
            "e48dd4954f614ad39151a2adf5b87858",
            "12007f64745c4b43a7d8c00b3b92ad2d"
          ]
        },
        "id": "uSt62gRXWVLP",
        "outputId": "43206b7f-7302-4d56-a600-08c4fe9c8bc3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ollama server running: 200\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ðŸ” Step 1: Extracting story specs...\n",
            "\n",
            "\n",
            "\n",
            "ðŸ§  Step 2: Generating outline...\n",
            "\n",
            "\n",
            "\n",
            "âœï¸ Step 3: Generating chapters...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ðŸ“š Generating Chapters:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25aac677146e483882d2b256cee3b384"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ðŸ“„ Step 4: Saving to Word document...\n",
            "âœ… Book saved to: /content/sample_data/generated_book.docx\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/generated_book.docx"
            ],
            "text/html": [
              "<a href='/content/sample_data/generated_book.docx' target='_blank'>/content/sample_data/generated_book.docx</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}