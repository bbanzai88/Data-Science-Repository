{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbanzai88/Data-Science-Repository/blob/main/syndromic_surveillance_colab_autoclamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXVB3T4l3vQb"
      },
      "source": [
        "# Syndromic Surveillance — Colab (Auto-Clamp Weeks/Days, NHSN + NWSS)\n",
        "\n",
        "This notebook is preconfigured to use **CDC NHSN weekly hospital admissions (COVID)** via Delphi Epidata (by state) and loads **CDC NWSS wastewater** data. It includes a helper that **auto-detects** whether your request is **weekly (epiweeks)** or **daily (YYYYMMDD)** and clamps the date range to an available window based on metadata.\n",
        "\n",
        "**What it does**\n",
        "1) Fetches NHSN weekly admissions (COVID) for CA/NY/TX\n",
        "2) Auto-clamps time range to the last N weeks/days that *exist* for your signal\n",
        "3) Runs a simple EARS detector with Benjamini–Hochberg FDR control\n",
        "4) Loads NWSS wastewater data and summarizes to weekly by state\n",
        "5) Plots trends and alerts; exports CSVs\n",
        "\n",
        "> *Note:* Alerts are statistical signals, not clinical diagnoses. Validate with multiple sources and tune thresholds for production use."
      ],
      "id": "EXVB3T4l3vQb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y1KjAz13vQd"
      },
      "source": [
        "#@title 0) Environment check (Colab-friendly)\n",
        "import sys, platform\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "print({\"python\": sys.version.split()[0], \"platform\": platform.platform(), \"IN_COLAB\": IN_COLAB})"
      ],
      "id": "2y1KjAz13vQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGEeVAgR3vQe"
      },
      "source": [
        "#@title 1) Install dependencies (avoid changing Colab's pandas/requests)\n",
        "!pip -q install -U delphi-epidata jupyter-dash duckdb python-dateutil tqdm rpy2 statsmodels plotly scipy epiweeks"
      ],
      "id": "zGEeVAgR3vQe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t6V4MGz3vQf"
      },
      "source": [
        "#@title 2) Imports & configuration (weekly NHSN defaults)\n",
        "import pandas as pd, numpy as np\n",
        "from datetime import datetime\n",
        "from dateutil import tz\n",
        "from tqdm import tqdm\n",
        "import duckdb\n",
        "from delphi_epidata import Epidata\n",
        "import plotly.express as px\n",
        "from jupyter_dash import JupyterDash\n",
        "from dash import Dash, dcc, html, dash_table\n",
        "\n",
        "CONFIG = {\n",
        "    \"source\": \"nhsn\",\n",
        "    \"signal\": \"confirmed_admissions_covid_ew\",\n",
        "    \"time_type\": \"week\",\n",
        "    \"geo_type\": \"state\",\n",
        "    \"geo_values\": [\"ca\", \"ny\", \"tx\"],\n",
        "    \"start\": 20210101,\n",
        "    \"end\":   int(datetime.now().strftime('%Y%m%d')),\n",
        "    \"ears_k\": 3,\n",
        "    \"ears_baseline\": 8,\n",
        "    \"apply_bh_fdr\": True,\n",
        "    \"alpha\": 0.05,\n",
        "    \"duckdb_path\": \"syndromic.duckdb\",\n",
        "    \"USE_R\": False\n",
        "}\n",
        "CONFIG"
      ],
      "id": "-t6V4MGz3vQf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5-ttMtR3vQf"
      },
      "source": [
        "#@title 3) Load Epidata metadata\n",
        "meta = Epidata.covidcast_meta()\n",
        "meta_df = pd.DataFrame(meta.get(\"epidata\", []))\n",
        "print(\"Total metadata rows:\", len(meta_df))\n",
        "display(meta_df.head(10))"
      ],
      "id": "o5-ttMtR3vQf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZ69xA63vQf"
      },
      "source": [
        "#@title 4) Auto-clamp helper (handles week/day automatically)\n",
        "from epiweeks import Week\n",
        "\n",
        "def _epi_to_week(ei: int) -> Week:\n",
        "    y, w = divmod(int(ei), 100)\n",
        "    return Week(y, w)\n",
        "\n",
        "def _week_to_epi(w: Week) -> int:\n",
        "    return w.year * 100 + w.week\n",
        "\n",
        "def auto_clamp_time_range(meta_df: pd.DataFrame, CONFIG: dict, last_n_days: int = 180, last_n_weeks: int = 104) -> None:\n",
        "    s, sg, tt, gt = CONFIG.get(\"source\"), CONFIG.get(\"signal\"), CONFIG.get(\"time_type\"), CONFIG.get(\"geo_type\")\n",
        "    mask = (\n",
        "        (meta_df[\"data_source\"] == s) &\n",
        "        (meta_df[\"signal\"]      == sg) &\n",
        "        (meta_df[\"time_type\"]   == tt) &\n",
        "        (meta_df[\"geo_type\"]    == gt)\n",
        "    )\n",
        "    subset = meta_df.loc[mask, [\"min_time\",\"max_time\"]]\n",
        "    if subset.empty:\n",
        "        print(f\"No metadata rows for {s}/{sg} ({tt}/{gt}). Leaving dates unchanged.\")\n",
        "        return\n",
        "    min_t = int(subset[\"min_time\"].min())\n",
        "    max_t = int(subset[\"max_time\"].max())\n",
        "    if tt == \"week\":\n",
        "        min_w = _epi_to_week(min_t)\n",
        "        max_w = _epi_to_week(max_t)\n",
        "        start_w = max_w - (last_n_weeks - 1)\n",
        "        if start_w < min_w:\n",
        "            start_w = min_w\n",
        "        CONFIG[\"start\"] = _week_to_epi(start_w)\n",
        "        CONFIG[\"end\"]   = max_t\n",
        "        print(f\"Auto-clamped (week): {CONFIG['start']} → {CONFIG['end']} (available {min_t} → {max_t})\")\n",
        "    else:\n",
        "        min_dt = pd.to_datetime(str(min_t), format=\"%Y%m%d\")\n",
        "        max_dt = pd.to_datetime(str(max_t), format=\"%Y%m%d\")\n",
        "        start_dt = max(min_dt, max_dt - pd.Timedelta(days=last_n_days))\n",
        "        CONFIG[\"start\"] = int(start_dt.strftime('%Y%m%d'))\n",
        "        CONFIG[\"end\"]   = max_t\n",
        "        print(f\"Auto-clamped (day): {CONFIG['start']} → {CONFIG['end']} (available {min_t} → {max_t})\")\n",
        "\n",
        "auto_clamp_time_range(meta_df, CONFIG, last_n_days=180, last_n_weeks=104)\n",
        "CONFIG"
      ],
      "id": "pCZ69xA63vQf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGi1TkEN3vQg"
      },
      "source": [
        "# 5) Robust fetch (legacy client + raw HTTP fallback)\n",
        "# Put this near your other helpers\n",
        "from epiweeks import Week\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_time_value(df: pd.DataFrame, time_type: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse df['time_value'] as datetime depending on time_type.\"\"\"\n",
        "    if \"time_value\" not in df:\n",
        "        return df\n",
        "\n",
        "    if time_type == \"week\":\n",
        "        # keep the original epiweek for reference\n",
        "        df[\"epiweek\"] = df[\"time_value\"].astype(int)\n",
        "        def epi_to_ts(x):\n",
        "            x = int(x)\n",
        "            y, w = divmod(x, 100)\n",
        "            return pd.Timestamp(Week(y, w).startdate())  # start of MMWR week (Monday)\n",
        "        df[\"time_value\"] = df[\"time_value\"].apply(epi_to_ts)\n",
        "    else:\n",
        "        # day: values are YYYYMMDD ints\n",
        "        df[\"time_value\"] = pd.to_datetime(df[\"time_value\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from delphi_epidata import Epidata\n",
        "\n",
        "def fetch_epidata_legacy(source: str, signal: str, time_type: str, geo_type: str,\n",
        "                         geo_values, start: int, end: int) -> pd.DataFrame:\n",
        "    if isinstance(geo_values, str):\n",
        "        geo_values = [geo_values]\n",
        "\n",
        "    # A) Legacy Python client (positional order)\n",
        "    try:\n",
        "        tv = Epidata.range(start, end)\n",
        "        resp = Epidata.covidcast(source, signal, time_type, geo_type, tv, geo_values)\n",
        "        if resp.get(\"result\") == 1 and resp.get(\"epidata\"):\n",
        "            df = pd.DataFrame(resp[\"epidata\"])\n",
        "            df = normalize_time_value(df, time_type)\n",
        "            return df\n",
        "    except Exception:\n",
        "        pass  # fall through to HTTP\n",
        "\n",
        "    # B) Raw HTTP fallback\n",
        "    url = \"https://api.delphi.cmu.edu/epidata/covidcast/\"\n",
        "    params = [\n",
        "        (\"data_source\", source), (\"signal\", signal),\n",
        "        (\"time_type\", time_type), (\"geo_type\", geo_type),\n",
        "        (\"time_values\", f\"{start}-{end}\")\n",
        "    ] + [(\"geo_value\", gv) for gv in geo_values]\n",
        "\n",
        "    r = requests.get(url, params=params, timeout=60)\n",
        "    js = r.json()\n",
        "    if js.get(\"result\") != 1 or not js.get(\"epidata\"):\n",
        "        raise RuntimeError(f\"No data for {source}/{signal} ({time_type}/{geo_type}) \"\n",
        "                           f\"{start}-{end}. API said: {js.get('message', js)}\")\n",
        "\n",
        "    df = pd.DataFrame(js[\"epidata\"])\n",
        "    df = normalize_time_value(df, time_type)\n",
        "    return df\n",
        "\n",
        "# re-run the fetch\n",
        "df = fetch_epidata_legacy(\n",
        "    CONFIG[\"source\"], CONFIG[\"signal\"], CONFIG[\"time_type\"],\n",
        "    CONFIG[\"geo_type\"], CONFIG[\"geo_values\"], CONFIG[\"start\"], CONFIG[\"end\"]\n",
        ")\n",
        "print(df.shape)\n",
        "display(df.head())\n"
      ],
      "id": "TGi1TkEN3vQg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-JXVYgY3vQg"
      },
      "source": [
        "# 6) EARS detector + BH-FDR\n",
        "def ears_detect(x: pd.DataFrame, value_col: str = \"value\", baseline: int = 8, k: float = 3.0) -> pd.DataFrame:\n",
        "    x = x.sort_values(\"time_value\").reset_index(drop=True).copy()\n",
        "    x[\"mu\"] = x[value_col].rolling(baseline, min_periods=baseline).mean().shift(1)\n",
        "    x[\"sd\"] = x[value_col].rolling(baseline, min_periods=baseline).std(ddof=1).shift(1)\n",
        "    eps = 1e-9\n",
        "    x[\"z\"] = (x[value_col] - x[\"mu\"]) / (x[\"sd\"].fillna(0) + eps)\n",
        "    from scipy.stats import norm\n",
        "    x[\"p\"] = 2 * (1 - norm.cdf(np.abs(x[\"z\"])) )\n",
        "    x[\"alert\"] = (x[\"z\"] >= k) & x[\"sd\"].notna()\n",
        "    return x\n",
        "\n",
        "def benjamini_hochberg(pvals: pd.Series, alpha=0.05) -> pd.Series:\n",
        "    p = np.asarray(pvals)\n",
        "    n = (~np.isnan(p)).sum()\n",
        "    if n == 0:\n",
        "        return pd.Series([False]*len(pvals), index=pvals.index)\n",
        "    order = np.argsort(p)\n",
        "    ranked = np.arange(1, n+1)\n",
        "    crit = alpha * ranked / n\n",
        "    p_sorted = p[order]\n",
        "    passed = p_sorted <= crit\n",
        "    kmax = ranked[passed].max() if passed.any() else 0\n",
        "    thresh = crit[kmax-1] if kmax>0 else -1\n",
        "    return pd.Series(pvals <= thresh, index=pvals.index)\n",
        "\n",
        "out = (df.groupby(\"geo_value\", group_keys=False)\n",
        "         .apply(lambda g: ears_detect(g, value_col=\"value\", baseline=CONFIG[\"ears_baseline\"], k=CONFIG[\"ears_k\"]))\n",
        "         .reset_index(drop=True))\n",
        "\n",
        "if CONFIG[\"apply_bh_fdr\"]:\n",
        "    def per_time_bh(g):\n",
        "        flags = benjamini_hochberg(g[\"p\"], alpha=CONFIG[\"alpha\"])\n",
        "        g[\"bh_discovery\"] = flags.values\n",
        "        return g\n",
        "    out = out.groupby(\"time_value\", group_keys=False).apply(per_time_bh)\n",
        "else:\n",
        "    out[\"bh_discovery\"] = out[\"alert\"]\n",
        "\n",
        "latest_date = out[\"time_value\"].max()\n",
        "alerts_latest = (out[out[\"time_value\"]==latest_date]\n",
        "                 .loc[:, [\"geo_value\",\"value\",\"mu\",\"sd\",\"z\",\"p\",\"alert\",\"bh_discovery\"]]\n",
        "                 .sort_values(\"z\", ascending=False))\n",
        "print(\"Latest period:\", latest_date)\n",
        "alerts_latest"
      ],
      "id": "T-JXVYgY3vQg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edvDOVci3vQh"
      },
      "source": [
        "# 7) Visualize trends & alerts (Plotly)\n",
        "def plot_geo(gv: str):\n",
        "    g = out.query(\"geo_value == @gv\").copy()\n",
        "    g[\"alert_flag\"] = np.where(g[\"bh_discovery\"], g[\"value\"], np.nan)\n",
        "    fig = px.line(g, x=\"time_value\", y=\"value\", title=f\"{gv.upper()} — {CONFIG['source']}/{CONFIG['signal']} ({CONFIG['time_type']})\")\n",
        "    fig.add_scatter(x=g[\"time_value\"], y=g[\"alert_flag\"], mode=\"markers\", name=\"ALERT (BH)\")\n",
        "    return fig\n",
        "\n",
        "for gv in CONFIG[\"geo_values\"]:\n",
        "    fig = plot_geo(gv)\n",
        "    fig.show()"
      ],
      "id": "edvDOVci3vQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OH3VVp3vQh"
      },
      "source": [
        "## NWSS (Wastewater) — ready-made pull (state-week summary)\n",
        "Loads CDC NWSS public table via Socrata (SODA). We auto-detect state/date/metric columns and build weekly medians by state."
      ],
      "id": "B1OH3VVp3vQh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYQByR7H3vQi"
      },
      "source": [
        "#@title 8) Load NWSS wastewater public data (CDC Socrata) — robust\n",
        "import pandas as pd\n",
        "\n",
        "# You can optionally narrow columns to reduce transfer:\n",
        "# NWSS_URL = (\"https://data.cdc.gov/resource/2ew6-ywp6.csv\"\n",
        "#             \"?$select=reporting_jurisdiction,date_end,date_start,percentile&$limit=500000\")\n",
        "NWSS_URL = \"https://data.cdc.gov/resource/2ew6-ywp6.csv?$limit=500000\"\n",
        "\n",
        "# Avoid DtypeWarning by letting pandas read in larger chunks\n",
        "ww = pd.read_csv(NWSS_URL, low_memory=False)\n",
        "print(\"NWSS rows, cols:\", ww.shape)\n",
        "display(ww.head(3))\n",
        "\n",
        "state_candidates = [\"reporting_jurisdiction\", \"state\", \"wwtp_jurisdiction\", \"jurisdiction\"]\n",
        "date_candidates  = [\n",
        "    \"date_end\", \"date_start\",                      # current schema\n",
        "    \"sample_collect_date\", \"sample_collection_date\",\n",
        "    \"collection_date\", \"first_sample_date\", \"date\", \"sample_date\"\n",
        "]\n",
        "metric_candidates = [\n",
        "    \"wastewater_percentile_7_day_rolling_average\",\n",
        "    \"wastewater_percentile\",\n",
        "    \"percentile\",\n",
        "    \"ww_percentile\"\n",
        "]\n",
        "\n",
        "def pick_col(df, candidates, kind):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    raise RuntimeError(f\"Could not find a {kind} column. Available: {list(df.columns)}\")\n",
        "\n",
        "state_col = pick_col(ww, state_candidates, \"state\")\n",
        "date_col  = pick_col(ww, date_candidates, \"date\")\n",
        "metric_col= pick_col(ww, metric_candidates, \"metric\")\n",
        "\n",
        "# Coerce types explicitly\n",
        "ww[date_col] = pd.to_datetime(ww[date_col], errors=\"coerce\")\n",
        "ww[metric_col] = pd.to_numeric(ww[metric_col], errors=\"coerce\")\n",
        "\n",
        "ww2 = ww[[state_col, date_col, metric_col]].dropna().copy()\n",
        "ww2 = ww2.rename(columns={state_col: \"state\", date_col: \"date\", metric_col: \"ww_percentile\"})\n",
        "\n",
        "# Aggregate to weekly-by-state (median) — uses calendar weeks; good enough for corroboration\n",
        "ww2[\"week\"] = ww2[\"date\"].dt.to_period(\"W\").dt.to_timestamp()\n",
        "ww_weekly = (ww2.groupby([\"state\", \"week\"], as_index=False)\n",
        "               .agg(ww_percentile=(\"ww_percentile\", \"median\")))\n",
        "\n",
        "print(\"Detected columns:\", {\"state\": state_col, \"date\": date_col, \"metric\": metric_col})\n",
        "print(ww_weekly.shape)\n",
        "display(ww_weekly.head(10))\n"
      ],
      "id": "MYQByR7H3vQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u3x433E3vQi"
      },
      "source": [
        "#@title 9) Store pulls & outputs in DuckDB (robust + collision-proof)\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "WRITE_MODE = \"append\"   # or \"replace\"\n",
        "SCHEMA = \"ss\"           # short schema name unlikely to collide with any attached catalog\n",
        "\n",
        "con = duckdb.connect(CONFIG[\"duckdb_path\"])\n",
        "\n",
        "# (Optional) show what catalogs are attached so you can spot collisions\n",
        "dbs = con.execute(\"PRAGMA database_list\").df()\n",
        "print(\"Databases attached:\")\n",
        "display(dbs)\n",
        "\n",
        "# Create our schema (no catalog prefix => use the connected DB)\n",
        "con.execute(f'CREATE SCHEMA IF NOT EXISTS \"{SCHEMA}\";')\n",
        "\n",
        "# Register DataFrames so DuckDB can read them\n",
        "con.register(\"df_epidata\", df)\n",
        "con.register(\"df_alerts\", out)\n",
        "con.register(\"df_nwss\", ww_weekly)\n",
        "\n",
        "if WRITE_MODE == \"replace\":\n",
        "    # Fresh snapshot each run\n",
        "    con.execute(f'CREATE OR REPLACE TABLE \"{SCHEMA}\".epidata AS SELECT * FROM df_epidata;')\n",
        "    con.execute(f'CREATE OR REPLACE TABLE \"{SCHEMA}\".alerts  AS SELECT * FROM df_alerts;')\n",
        "    con.execute(f'CREATE OR REPLACE TABLE \"{SCHEMA}\".nwss    AS SELECT * FROM df_nwss;')\n",
        "else:\n",
        "    # Create tables from the DataFrame schemas, then append by column name\n",
        "    con.execute(f'CREATE TABLE IF NOT EXISTS \"{SCHEMA}\".epidata AS SELECT * FROM df_epidata LIMIT 0;')\n",
        "    con.execute(f'CREATE TABLE IF NOT EXISTS \"{SCHEMA}\".alerts  AS SELECT * FROM df_alerts   LIMIT 0;')\n",
        "    con.execute(f'CREATE TABLE IF NOT EXISTS \"{SCHEMA}\".nwss    AS SELECT * FROM df_nwss     LIMIT 0;')\n",
        "\n",
        "    con.execute(f'INSERT INTO \"{SCHEMA}\".epidata BY NAME SELECT * FROM df_epidata;')\n",
        "    con.execute(f'INSERT INTO \"{SCHEMA}\".alerts  BY NAME SELECT * FROM df_alerts;')\n",
        "    con.execute(f'INSERT INTO \"{SCHEMA}\".nwss    BY NAME SELECT * FROM df_nwss;')\n",
        "\n",
        "# Quick counts for sanity\n",
        "counts = con.execute(f\"\"\"\n",
        "  SELECT 'epidata' AS table, COUNT(*) AS rows FROM \"{SCHEMA}\".epidata\n",
        "  UNION ALL SELECT 'alerts', COUNT(*) FROM \"{SCHEMA}\".alerts\n",
        "  UNION ALL SELECT 'nwss',   COUNT(*) FROM \"{SCHEMA}\".nwss\n",
        "\"\"\").df()\n",
        "\n",
        "con.close()\n",
        "print(\"Saved to\", CONFIG[\"duckdb_path\"])\n",
        "display(counts)\n",
        "\n",
        "\n"
      ],
      "id": "2u3x433E3vQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBP5hsAF3vQj"
      },
      "source": [
        "#@title 10) Minimal in-notebook dashboard (Plotly + ipywidgets; Dash-free)\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def figure_for(gv: str):\n",
        "    g = out.query(\"geo_value == @gv\").copy()\n",
        "    g[\"alert_flag\"] = np.where(g[\"bh_discovery\"], g[\"value\"], np.nan)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=g[\"time_value\"], y=g[\"value\"], mode=\"lines\", name=\"value\"))\n",
        "    fig.add_trace(go.Scatter(x=g[\"time_value\"], y=g[\"alert_flag\"], mode=\"markers\", name=\"ALERT (BH)\"))\n",
        "    fig.update_layout(\n",
        "        title=f\"{gv.upper()} — {CONFIG['source']}/{CONFIG['signal']} ({CONFIG['time_type']})\",\n",
        "        xaxis_title=\"time\",\n",
        "        yaxis_title=\"value\",\n",
        "        legend_title=\"\"\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "# Try interactive dropdown; fall back to static if ipywidgets unavailable\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "\n",
        "    geo_opts = [(g.upper(), g) for g in CONFIG[\"geo_values\"]]\n",
        "    dd = widgets.Dropdown(options=geo_opts, value=CONFIG[\"geo_values\"][0], description=\"Geo:\")\n",
        "    out_box = widgets.Output()\n",
        "\n",
        "    def render(_=None):\n",
        "        with out_box:\n",
        "            clear_output(wait=True)\n",
        "            fig = figure_for(dd.value)\n",
        "            fig.show()\n",
        "\n",
        "    dd.observe(render, names=\"value\")\n",
        "    display(widgets.VBox([dd, out_box]))\n",
        "    render()  # initial render\n",
        "\n",
        "    # Also show latest-period alerts table below\n",
        "    import pandas as pd\n",
        "    display(alerts_latest)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"ipywidgets unavailable — showing static charts for each geo.\\nReason:\", e)\n",
        "    for gv in CONFIG[\"geo_values\"]:\n",
        "        fig = figure_for(gv)\n",
        "        fig.show()\n",
        "    display(alerts_latest)\n"
      ],
      "id": "IBP5hsAF3vQj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9unkbcc3vQj"
      },
      "source": [
        "#@title 11) Export CSVs (alerts & raw pulls)\n",
        "from datetime import datetime\n",
        "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "raw_path = f\"epidata_NHSN_raw_{ts}.csv\"\n",
        "alerts_path = f\"alerts_NHSN_{ts}.csv\"\n",
        "nwss_path = f\"nwss_weekly_{ts}.csv\"\n",
        "df.to_csv(raw_path, index=False)\n",
        "out.to_csv(alerts_path, index=False)\n",
        "ww_weekly.to_csv(nwss_path, index=False)\n",
        "raw_path, alerts_path, nwss_path"
      ],
      "id": "H9unkbcc3vQj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}