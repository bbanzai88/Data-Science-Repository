{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqZVdhGjK6KEQs5D36zDl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbanzai88/Data-Science-Repository/blob/main/Thomas_Heiman_Resume_Chatbot_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an experiment in using a chatbot to query my resume. I am using the following packages:üí¨\n",
        "\n",
        "FAISS üë©‚Äçüíª Facebook AI Similarity Search is a popular library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. We will use it to quickly search through CVs and recommendation letters to find relevant text fragments.\n",
        "\n",
        "LangChain ü¶úüîó LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It‚Äôs use-cases overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
        "\n",
        "This notebook based upon the medium article at:https://blog.gopenai.com/transform-your-cv-into-an-interactive-chatbot-with-llm-faiss-and-langchain-64263241d46d\n",
        "\n",
        "I have updated it to use cpu only as the requirements for the faiss-gpu changed and made the whole notebook unusable."
      ],
      "metadata": {
        "id": "scgT8uca8PQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install dependencies\n",
        "!pip install --quiet faiss-cpu langchain langchain-community transformers sentence-transformers pypdf\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# STEP 3: Upload your resume (PDF)\n",
        "print(\"üìÑ Upload your resume PDF\")\n",
        "uploaded = files.upload()\n",
        "resume_path = list(uploaded.keys())[0]\n",
        "\n",
        "# STEP 4: Load and split the resume into chunks\n",
        "loader = PyPDFLoader(resume_path)\n",
        "pages = loader.load()\n",
        "\n",
        "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=500, chunk_overlap=50)\n",
        "docs = splitter.split_documents(pages)\n",
        "\n",
        "# STEP 5: Create embeddings and FAISS index\n",
        "print(\"üîç Creating vector index from resume...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Step 6: Load LLM (lightweight and fast)\n",
        "print(\"‚öôÔ∏è Loading LLM...\")\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"distilgpt2\",\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=150,\n",
        "    pad_token_id=50256\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "# STEP 7: Set up Retrieval QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever()\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "# Step 8: Start interactive Q&A\n",
        "def chat():\n",
        "    print(\"\\nüí¨ Ask me anything about your resume! (type 'exit' to quit)\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"You: \")\n",
        "        except EOFError:\n",
        "            break\n",
        "        if query.lower() in ['exit', 'quit']:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "        result = qa_chain.invoke(query)\n",
        "        print(\"ü§ñ Bot:\", result)\n",
        "chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "B_3QKhV840JC",
        "outputId": "2aec9ed7-e271-4274-9148-05e54719e626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Upload your resume PDF\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fb5515af-2f41-4b3f-bf4b-1e7cfca92b5e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fb5515af-2f41-4b3f-bf4b-1e7cfca92b5e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TomHeiman_Resume_New.pdf to TomHeiman_Resume_New (2).pdf\n",
            "üîç Creating vector index from resume...\n",
            "‚öôÔ∏è Loading LLM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí¨ Ask me anything about your resume! (type 'exit' to quit)\n",
            "You: Is Thomas Heiman a good scientist?\n",
            "ü§ñ Bot: {'query': 'Is Thomas Heiman a good scientist?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\ninitiatives across federal agencies including the FDA, DHS, USCIS, and DoD. \\nHolds a Ph.D. in Computational Biology and Bioinformatics and a proven track \\nrecord in machine learning, data engineering, scientific programming, and \\nregulatory data science. Skilled in R, Python, SQL, and MATLAB, with deep \\nexpertise in entity resolution, topic modeling, and time -series forecasting. \\nPublished contributor to Science and recognized AI reviewer and educator.  \\nPROFESSIONAL EXPERIENCE\\n\\n703-303-3517 theiman@verizon.net Reston, Virginia \\n \\n \\n \\n \\n \\nEDUCATION \\nGeorge Mason University \\nPh.D. in Computational Biology \\nand Bioinformatics \\nM.S. in Computational Sciences \\nUniversity of Arizona \\nB.A. in Geology (Concentration in \\nGeochemistry) \\n \\nTECHNICAL SKILLS \\nAdvanced Scripting : R, Python, \\nSQL, MATLAB \\nData Mining Tools : SPSS \\nClementine, RapidMiner, \\nKNIME \\nProgramming Languages & \\nEnvironments: Mathematica, \\nSAS, Oracle SQL, HTML \\nML/AI Platforms: Databricks\\n\\nNavy, and SEC. \\nText Analytics SME / Bioinformatics Specialist \\nEngility Corporation / FDA ‚Äì Reston, VA | 2014 ‚Äì 2017 \\nBuilt CAR-T toxicity prediction models and R Shiny dashboards. \\nPrototyped document triage pipelines and standardized health data processing flows. \\nThomas J. Heiman, Ph.D. \\nSenior Data Scientist | AI/ML Researcher | Bioinformatics Expert\\n\\nBuilt an OCR pipeline for passport MRZ using OpenCV and image quality \\nassessments. \\nPredicted USCIS form processing times using XGBoost, DNNs, and AutoKeras. \\nSenior Data Scientist \\nStrategamy ‚Äì Dayton, MD | Apr 2021 ‚Äì Sep 2022 \\nDeveloped knowledge graph solutions for FDA ORA, integrating PubChem and \\nPubMed data. \\nCreated AI Playbook for project managers across six AI/ML competencies. \\nSenior Data Scientist \\nTechnogems ‚Äì Reston, VA | Feb 2020 ‚Äì Dec 2020\\n\\nQuestion: Is Thomas Heiman a good scientist?\\nHelpful Answer: Thomas Heiman is a good scientist. \\nLack of knowledge\\n\\nR Shiny dashboards and a full suite of tools for developing and implementing complex, complex, complex, complex, complex, complex, complex and complex complex code. \\nSenior Data Scientist \\nSAS, Oracle SQL, SQL, and MATLAB\\nLack of knowledge\\nR Shiny dashboards and a full suite of tools for developing and implementing complex, complex, complex, complex, complex and complex code. \\nSenior Data Scientist \\nIntelligent Systems Engineer\\nAdvanced Design\\n\\nThe University is the home of the University of Virginia Computer Science and Engineering, the largest private research and development training facility in the U.S. and the largest private research and\"}\n",
            "You: Dont know\n",
            "ü§ñ Bot: {'query': 'Dont know', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nBuilt an OCR pipeline for passport MRZ using OpenCV and image quality \\nassessments. \\nPredicted USCIS form processing times using XGBoost, DNNs, and AutoKeras. \\nSenior Data Scientist \\nStrategamy ‚Äì Dayton, MD | Apr 2021 ‚Äì Sep 2022 \\nDeveloped knowledge graph solutions for FDA ORA, integrating PubChem and \\nPubMed data. \\nCreated AI Playbook for project managers across six AI/ML competencies. \\nSenior Data Scientist \\nTechnogems ‚Äì Reston, VA | Feb 2020 ‚Äì Dec 2020\\n\\nACTIVITIES & \\nAFFILIATIONS \\nReviewer: O'Reilly, Manning, \\nPacket titles on AI, ML, Causal \\nInference, Robotics (2014‚Äì2024) \\nProgram Committee: IEEE \\nICDM (2008‚Äì2011) \\n \\nSECURITY CLEARANCE \\nPast Clearances (All Expired): \\nIRS MBI \\nDoD Secret \\nDHS Suitability \\nFDA \\nEligible to re-obtain clearance as \\nneeded. \\nSenior Analytical Consultant \\nDun G Bradstreet | 2012 ‚Äì 2013 \\nPredicted inactive business locations for FDA to improve field inspection efficiency. \\n \\nConsultant ‚Äì Multiple Projects\\n\\nUNLV, Pepperdine, University of New Jersey, HackerUSA and others. \\nPUBLICATIONS \\nYonatan Negash, Thomas Heiman, Judith Crumpler, Bindu George, Maura O‚ÄôLeary, \\nKristin Baird, Robert Sokolic, Kimberly Schultz, John Scott, Vahan Simonyan: CAR-T \\nSafety Database Pilot Project. 2016 FDA Scientific Computing Day.  \\nWeizhong Zhao, Yijun Ding, Ke Yu, Wen Zou, James J. Chen, Weida Tong, Roger \\nPerkins, Deborah Sholtes, Wendy Aaronson, Thomas Heiman, Judith Crumpler: Best\\n\\nPROFESSIONAL EXPERIENCE \\nSenior Data Scientist \\n22nd Century Technologies ‚Äì McLean, VA | Dec 2023 ‚Äì Present \\nEngineered memory-efficient deduplication pipelines for LA County Public Health \\nusing fuzzy matching and hashing techniques.  \\nDesigned evaluation frameworks for record linkage efficacy and mentored junior data \\nscientists. \\nCreated training material on R/Posit Workbench. \\nSenior Data Scientist \\nHighlight Technologies ‚Äì Reston, VA | Nov 2022 ‚Äì Sep 2023\\n\\nQuestion: Dont know\\nHelpful Answer: \\nDo you know the answer? I am not sure but it is certainly possible as a result of my experience in the field. \\nMy experience in the field has been highly-visited in the field and I have seen so many examples of how I have been able to improve the use of this kind of technology. \\nThis is important because I want to help the field be aware of the pitfalls of this technology. \\nIt is important because I want to help the field be aware of the pitfalls of this technology. \\nI do not want to help the field be aware of the pitfalls of this technology. \\nThere is no more need to be aware of the pitfalls of this technology. \\nI have never seen\"}\n",
            "You: Who is Tom?\n",
            "ü§ñ Bot: {'query': 'Who is Tom?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\ninitiatives across federal agencies including the FDA, DHS, USCIS, and DoD. \\nHolds a Ph.D. in Computational Biology and Bioinformatics and a proven track \\nrecord in machine learning, data engineering, scientific programming, and \\nregulatory data science. Skilled in R, Python, SQL, and MATLAB, with deep \\nexpertise in entity resolution, topic modeling, and time -series forecasting. \\nPublished contributor to Science and recognized AI reviewer and educator.  \\nPROFESSIONAL EXPERIENCE\\n\\n703-303-3517 theiman@verizon.net Reston, Virginia \\n \\n \\n \\n \\n \\nEDUCATION \\nGeorge Mason University \\nPh.D. in Computational Biology \\nand Bioinformatics \\nM.S. in Computational Sciences \\nUniversity of Arizona \\nB.A. in Geology (Concentration in \\nGeochemistry) \\n \\nTECHNICAL SKILLS \\nAdvanced Scripting : R, Python, \\nSQL, MATLAB \\nData Mining Tools : SPSS \\nClementine, RapidMiner, \\nKNIME \\nProgramming Languages & \\nEnvironments: Mathematica, \\nSAS, Oracle SQL, HTML \\nML/AI Platforms: Databricks\\n\\nNavy, and SEC. \\nText Analytics SME / Bioinformatics Specialist \\nEngility Corporation / FDA ‚Äì Reston, VA | 2014 ‚Äì 2017 \\nBuilt CAR-T toxicity prediction models and R Shiny dashboards. \\nPrototyped document triage pipelines and standardized health data processing flows. \\nThomas J. Heiman, Ph.D. \\nSenior Data Scientist | AI/ML Researcher | Bioinformatics Expert\\n\\nTechnogems ‚Äì Reston, VA | Feb 2020 ‚Äì Dec 2020 \\nLed metrics development and model evaluation strategies for USCIS entity \\nresolution. \\nSenior Data Scientist \\nSevatec ‚Äì Fairfax, VA | Feb 2018 ‚Äì Feb 2020 \\nDeveloped ML models to predict citizenship appointment no -shows and time-series \\nforecasts for application volumes.  \\nConsultant (Various Roles) \\nSAIC, Harmonia, Engility, Prevalent | 2017‚Äì2021 \\nProvided NLP, optimization, and generative modeling solutions for projects in FDA, \\nNavy, and SEC.\\n\\nQuestion: Who is Tom?\\nHelpful Answer:\\nTom is a former Vice President of the National Science Foundation. He is the President of the National Science Foundation and a member of the National Science Foundation's Annual Meeting of the National Academy of Sciences. He is also the author of the book, ‚Ä≤The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea,‚Ä≤ The Big Idea\"}\n",
            "You: What are toms strengths?\n",
            "ü§ñ Bot: {'query': 'What are toms strengths?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nNavy, and SEC. \\nText Analytics SME / Bioinformatics Specialist \\nEngility Corporation / FDA ‚Äì Reston, VA | 2014 ‚Äì 2017 \\nBuilt CAR-T toxicity prediction models and R Shiny dashboards. \\nPrototyped document triage pipelines and standardized health data processing flows. \\nThomas J. Heiman, Ph.D. \\nSenior Data Scientist | AI/ML Researcher | Bioinformatics Expert\\n\\n703-303-3517 theiman@verizon.net Reston, Virginia \\n \\n \\n \\n \\n \\nEDUCATION \\nGeorge Mason University \\nPh.D. in Computational Biology \\nand Bioinformatics \\nM.S. in Computational Sciences \\nUniversity of Arizona \\nB.A. in Geology (Concentration in \\nGeochemistry) \\n \\nTECHNICAL SKILLS \\nAdvanced Scripting : R, Python, \\nSQL, MATLAB \\nData Mining Tools : SPSS \\nClementine, RapidMiner, \\nKNIME \\nProgramming Languages & \\nEnvironments: Mathematica, \\nSAS, Oracle SQL, HTML \\nML/AI Platforms: Databricks\\n\\ninitiatives across federal agencies including the FDA, DHS, USCIS, and DoD. \\nHolds a Ph.D. in Computational Biology and Bioinformatics and a proven track \\nrecord in machine learning, data engineering, scientific programming, and \\nregulatory data science. Skilled in R, Python, SQL, and MATLAB, with deep \\nexpertise in entity resolution, topic modeling, and time -series forecasting. \\nPublished contributor to Science and recognized AI reviewer and educator.  \\nPROFESSIONAL EXPERIENCE\\n\\nTechnogems ‚Äì Reston, VA | Feb 2020 ‚Äì Dec 2020 \\nLed metrics development and model evaluation strategies for USCIS entity \\nresolution. \\nSenior Data Scientist \\nSevatec ‚Äì Fairfax, VA | Feb 2018 ‚Äì Feb 2020 \\nDeveloped ML models to predict citizenship appointment no -shows and time-series \\nforecasts for application volumes.  \\nConsultant (Various Roles) \\nSAIC, Harmonia, Engility, Prevalent | 2017‚Äì2021 \\nProvided NLP, optimization, and generative modeling solutions for projects in FDA, \\nNavy, and SEC.\\n\\nQuestion: What are toms strengths?\\nHelpful Answer: For example, use the term ‚Äªs to describe the different types of jobs a job is currently in. For example, an employee in a particular company would have to work in three separate jobs, and a job in multiple companies would have to work in one job. For example, an employee in a particular company would have to work in one job. A person can be a product manager on a company-by-company basis, and the person who would work in the same job would have to work in multiple companies. For example, an employee in a particular company would have to work in two separate jobs, and the person who would work in a different job would have to work in one job. For example, an employee in a particular company\"}\n"
          ]
        }
      ]
    }
  ]
}